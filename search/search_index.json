{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metagenomics Workshop Overview","text":""},{"location":"#course-overview","title":"Course overview","text":"<p>This course will present the fundamentals of metagenomic analysis.</p>"},{"location":"#setup","title":"Setup","text":"<p>The data used in this workshop are available on Zenodo. Please read the Zenodo page linked below for information about the data and access to the data files. Because this workshop works with real data, be aware that file sizes for the data are large. (1 Gb) https://zenodo.org/records/7010950</p>"},{"location":"#schedule","title":"Schedule","text":"<p>You can find the course schedule at this page.</p>"},{"location":"#contact","title":"Contact","text":"<p>To contact me, even after the training, please send a mail to mathis.arnal@ird.fr </p>"},{"location":"pages/take_down/","title":"Take down","text":"<p>Depending on which of the tutorials you have taken, there might be quite a lot of files stored on your computer. Here are instructions for how to remove them.</p> <p>All the tutorials depend on you cloning the <code>workshop-reproducible-research</code> GitHub repo. This can be removed like any other directory; via Finder, Explorer or <code>rm -rf workshop-reproducible-research</code>. Note that this will also delete the hidden directories <code>.git</code>, which contains the history of the repo, and <code>.snakemake</code>, which contains the history of any Snakemake runs.</p>"},{"location":"pages/take_down/#conda","title":"Conda","text":"<p>Several of the tutorials use Conda for installing packages. This amounts to about 2.6 GB if you've done all the tutorials. If you plan on using Conda in the future you can remove just the packages, or you can remove everything including Conda itself. Note that this is not needed if you've done the tutorials on Windows using Docker (see the section on Docker below instead).</p> <p>In order to remove all your Conda environments, you first need to list them:</p> <pre><code>conda env list\n</code></pre> <p>For each of the environments except \"base\" run the following:</p> <pre><code>conda remove -n envname --all\n</code></pre> <p>And, finally:</p> <pre><code>conda clean --all\n</code></pre> <p>If you also want to remove Conda itself (i.e. removing all traces of Conda), you need to check where Conda is installed. Look for the row \"base environment\".</p> <pre><code>conda info\n</code></pre> <p>This should say something like <code>/Users/&lt;user&gt;/miniconda3</code>. Then remove the entire Conda directory:</p> <pre><code>rm -rf /Users/&lt;user&gt;/miniconda3\n</code></pre> <p>Lastly, open your <code>~/.bashrc</code> file (or <code>~/.bash_profile</code> if on Mac) in a text editor and remove the path to Conda from PATH.</p>"},{"location":"pages/take_down/#snakemake","title":"Snakemake","text":"<p>Snakemake is installed via Conda and will be removed if you follow the instructions in the Conda section above. Note that Snakemake also generates a hidden <code>.snakemake</code> directory in the directory where it's run. You can remove this with the following:</p> <pre><code>rm -rf workshop-reproducible-research/snakemake/.snakemake\n</code></pre>"},{"location":"pages/take_down/#jupyter","title":"Jupyter","text":"<p>Jupyter is installed via Conda and will be removed if you follow the instructions in the Conda section above.</p>"},{"location":"pages/take_down/#docker","title":"Docker","text":"<p>If you've done the Docker tutorial or if you've been running Docker for Windows you have some cleaning up to do. Docker is infamous for quickly taking up huge amounts of space, and some maintenance is necessary every now and then. Here is how to uninstall Docker completely. For instructions for how to remove individual images or containers, see the Docker tutorial.</p>"},{"location":"pages/take_down/#macos","title":"macOS","text":"<p>Click the Docker icon in the menu bar (upper right part of the screen) and select \"Preferences\". In the upper right corner, you should find a little bug icon. Click on that icon and select \"Reset to factory defaults\". You may have to fill  in your password. Then select \"Uninstall\". Once it's done uninstalling, drag the  Docker app from Applications to Trash.</p>"},{"location":"pages/take_down/#linux","title":"Linux","text":"<p>If you've installed Docker with <code>apt-get</code>, uninstall it like this:</p> <pre><code>apt-get purge docker-ce\n</code></pre> <p>Images, containers, and volumes are not automatically removed. To delete all of them:</p> <pre><code>rm -rf /var/lib/docker\n</code></pre>"},{"location":"pages/take_down/#windows","title":"Windows","text":"<p>Uninstall Docker for Windows (on Windows 10) or Docker Toolbox (on Windows 7) via Control Panel &gt; Programs &gt; Programs and Features. Docker Toolbox will also have installed Oracle VM VirtualBox, so uninstall that as well if you're not using it for other purposes.</p>"},{"location":"pages/take_down/#singularity","title":"Singularity","text":"<p>Singularity images are files that can simply be deleted. Singularity also creates a hidden directory <code>.singularity</code> in your home directory that contains its cache, which you may delete.</p>"},{"location":"pages/take_down/#windows_1","title":"Windows","text":"<p>On Windows, you will additionally need to uninstall Git for Windows, VirtualBox, Vagrant and Vagrant Manager (see the  Singularity installation guide).</p>"},{"location":"pages/course-information/code-of-conduct/","title":"SouthGreen Training Code of Conduct","text":"<p>Training is one of the core values of SouthGreen, and benefits from the contributions of the entire scientific community. We value the involvement of everyone in the community. We are committed to creating a friendly and respectful place for learning, teaching and contributing. All participants in our events and communications are expected to show respect and courtesy to others.</p> <p>To make clear what is expected, everyone participating in SouthGreen courses is required to conform to the Code of Conduct. This Code of Conduct applies to all spaces managed by SouthGreen including, but not limited to, courses, email lists, and online forums such as Studium, GitHub, Slack, Twitter and LinkedIn. Course organizers and teachers are expected to assist with the enforcement of the Code of Conduct.</p> <p>We are dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. By participating in this event, participants accept to abide by the SouthGreen Code of Conduct and accept the procedures by which any Code of Conduct incidents are resolved. Any form of behaviour to exclude, intimidate, or cause discomfort is a violation of the Code of Conduct. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all platforms and training events:</p> <ul> <li>Use welcoming and inclusive language</li> <li>Be respectful of different viewpoints and experiences</li> <li>Gracefully accept constructive criticism</li> <li>Focus on what is best to all those involved in this training event</li> <li>Show courtesy and respect towards everyone involved in this training event</li> </ul> <p>For an extended description please see the ELIXIR Code of Conduct.</p>"},{"location":"pages/course-information/lectures/","title":"Lectures","text":"<p>This page contains links to all the lectures in the course in PDF format, which you can either view in your browser (click the link) or download (right-click and save the link to wherever you want it). If you want to view the raw R Markdown / Jupyter files you can find them at GitHub, along with instructions on how to render them.</p>"},{"location":"pages/course-information/lectures/#lecture-links","title":"Lecture links","text":"<ul> <li>Introduction</li> <li>Data management</li> <li>Git</li> <li>Conda</li> <li>R Markdown</li> <li>Jupyter</li> <li>Snakemake</li> <li>Nextflow</li> <li>Containers</li> <li>Putting it together</li> </ul>"},{"location":"pages/course-information/location/","title":"Location","text":"<p>Put Information about:   * How to get there   * Name of the room   * Badge needed? Campus map?   * etc....</p>"},{"location":"pages/course-information/pre-course-setup/","title":"Pre course setup","text":"<p>All of the tutorials and the material in them is dependent on the GitHub repository for the course. The first step of the setup is thus to download all the files that you will need, which is done differently depending on which operating system you have.</p> <p>At the last day, you will have the opportunity to try out the different tools on one of your own projects. In case you don't want to use a project you are currently working on, we have prepared a small-scale project for you. If you would like to work on your own project, it would be great if you could have the code and data ready before the end of the course so that you have more time for the exercise. In case your analysis project contains computationally intense steps it may be good to scale them down for the sake of the exercise. You might, for example, subset your raw data to only contain a minuscule part of its original size.</p>"},{"location":"pages/course-information/pre-course-setup/#setup-for-mac-linux-users","title":"Setup for Mac / Linux users","text":"<p>First, <code>cd</code> into a directory on your computer (or create one) where it makes sense to download the course directory.</p> <pre><code>cd /path/to/your/directory\ngit clone https://github.com/NBISweden/workshop-reproducible-research.git\ncd workshop-reproducible-research\n</code></pre> <p>Tip</p> <p>If you want to revisit the material from an older instance of this course, you can do that using <code>git checkout tags/&lt;tag-name&gt;</code>, e.g. <code>git checkout tags/course_1905</code>. To list all available tags, use <code>git tag</code>. Run this command after you have <code>cd</code> into <code>workshop-reproducible-research</code> as described above. If you do that, you probably also want to view the same older version of this website. Until spring 2021, the website was hosted at https://nbis-reproducible-research.readthedocs.io. Locate the version box in the bottom right corner of the website and select the corresponding version.</p>"},{"location":"pages/course-information/pre-course-setup/#setup-for-windows-users","title":"Setup for Windows users","text":"<p>Using a Windows computer for bioinformatic work has sadly not been ideal most of the time, but large advanced in recent years have made this quite feasible through the Windows 10 Linux subsystem. This is the only setup for Windows users that we allow for participants of this course, as all the material has been created and tested to work on Unix-based systems.</p> <p>Using the Linux subsystem will give you access to a full command-line bash shell based on Linux on your Windows 10 PC. For the difference between the Linux Bash Shell and the PowerShell on Windows 10, see e.g. this article.</p> <p>Install Bash on Windows 10, follow the instructions at e.g. one of these resources:</p> <ul> <li>Installing the Windows Subsystem and the Linux Bash</li> <li>Installing and using Linux Bash on Windows</li> <li>Installing Linux Bash on Windows</li> </ul> <p>Note</p> <p>If you run into error messages when trying to download files through the Linux shell (e.g. <code>curl:(6) Could not resolve host</code>) then try adding the Google nameserver to the internet configuration by running <code>sudo nano /etc/resolv.conf</code> then add <code>nameserver 8.8.8.8</code> to the bottom of the file and save it.</p> <p>Open a bash shell Linux terminal and clone the GitHub repository containing all files you will need for completing the tutorials as follows. First, <code>cd</code> into a directory on your computer (or create one) where it makes sense to download the course directory.</p> <p>Tip</p> <p>You can find the directory where the Linux distribution is storing all its files by typing <code>explorer.exe .</code>. This will launch the Windows File Explorer showing the current Linux directory. Alternatively, you can find the Windows C drive from within the bash shell Linux terminal by navigating to <code>/mnt/c/</code>.</p> <pre><code>cd /path/to/your/directory\ngit clone https://github.com/NBISweden/workshop-reproducible-research.git\ncd workshop-reproducible-research\n</code></pre> <p>Whenever a setup instruction specifies Mac or Linux (i.e. only those two, with no alternative for Windows), please follow the Linux instructions.</p> <p>Tip</p> <p>If you want to revisit the material from an older instance of this course, you can do that using <code>git checkout tags/&lt;tag-name&gt;</code>, e.g. <code>git checkout tags/course_1905</code>. To list all available tags, use <code>git tag</code>. Run this command after you have <code>cd</code> into <code>workshop-reproducible-research</code> as described above. If you do that, you probably also want to view the same older version of this website. Until spring 2021, the website was hosted at https://nbis-reproducible-research.readthedocs.io/en/latest/. Locate the version box in the bottom right corner of the website and select the corresponding version.</p>"},{"location":"pages/course-information/pre-course-setup/#installing-git","title":"Installing Git","text":"<p>Chances are that you already have git installed on your computer. You can check by running e.g. <code>git --version</code>. If you don't have git, install it following the instructions here. If you have a very old version of git you might want to update to a later version.</p>"},{"location":"pages/course-information/pre-course-setup/#configure-git","title":"Configure git","text":"<p>If it is the first time you use git on your computer, you may want to configure it so that it is aware of your username and email. These should match those that you have registered on GitHub. This will make it easier when you want to sync local changes with your remote GitHub repository.</p> <pre><code>git config --global user.name \"Mona Lisa\"\ngit config --global user.email \"mona_lisa@gmail.com\"\n</code></pre> <p>Tip</p> <p>If you have several accounts (e.g. both a GitHub and Bitbucket account), and thereby several different usernames, you can configure git on a per-repository level. Change directory into the relevant local git repository and run <code>git config user.name \"Mona Lisa\"</code>. This will set the default username for that repository only.</p> <p>You will also need to configure the default branch name to be <code>main</code> instead of <code>master</code>:</p> <pre><code>git config --global init.defaultBranch \"main\"\n</code></pre> <p>The short version of why you need to do this is that GitHub uses <code>main</code> as the default branch while Git itself is still using <code>master</code>; please read the box below for more information.</p> <p>Note</p> <p>The default branch name for Git and many of the online resources for hosting Git repositories has traditionally been <code>master</code>, which historically comes from the \"master/slave\" repositories of BitKeeper. This has been heavily discussed and in 2020 the decision was made by  many (including GitHub) to start using <code>main</code> instead. Any repository created with GitHub uses this new naming scheme since October of 2020, and Git itself is currently discussing implementing a similar change. Git did, however, introduce the ability to set the default branch name when using <code>git init</code> in version 2.28, instead of using a hard-coded <code>master</code>. We at NBIS want to be a part of this change, so we have chosen to use <code>main</code> for this course.</p>"},{"location":"pages/course-information/pre-course-setup/#installing-conda","title":"Installing Conda","text":"<p>Conda is installed by downloading and executing an installer from the Conda website, but which version you need depends on your operating system:</p> <pre><code># Install Miniconda3 for 64-bit Mac\ncurl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-MacOSX-x86_64.sh -O\nbash Miniconda3-4.7.12.1-MacOSX-x86_64.sh\nrm Miniconda3-4.7.12.1-MacOSX-x86_64.sh\n</code></pre> <pre><code># Install Miniconda3 for 64-bit Linux\ncurl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O\nbash Miniconda3-4.7.12.1-Linux-x86_64.sh\nrm Miniconda3-4.7.12.1-Linux-x86_64.sh\n</code></pre> <p>Warning</p> <p>If you already have installed Conda but want to update, you should be able to simply run <code>conda update conda</code> and subsequently <code>conda init</code>, and skip the installation instructions below.</p> <p>Warning</p> <p>If you have a newer Apple computer with an M1 chip, make sure you have installed Rosetta before you run the installer. If you want to more fully utilise the new architecture, head over to Miniforge!</p> <p>The installer will ask you questions during the installation:</p> <ul> <li>Do you accept the license terms? (Yes)</li> <li>Do you accept the installation path or do you want to choose a different one?   (Probably yes)</li> <li>Do you want to run <code>conda init</code> to setup Conda on your system? (Yes)</li> </ul> <p>Restart your shell so that the settings in <code>~/.bashrc</code>/<code>~/.bash_profile</code> can take effect. You can verify that the installation worked by running:</p> <pre><code>conda --version\n</code></pre> <p>Different Condas  There are three Conda-related things you may have encountered: the first is Conda, the package and environment manager we've been talking about so far. Second is Miniconda, which is the installer for Conda. The third is Anaconda, which is a distribution of not only Conda, but also over 150 scientific Python packages. It's generally better to stick with only Conda, i.e. installing with Miniconda, rather than installing 3 GB worth of packages you may not even use.</p>"},{"location":"pages/course-information/pre-course-setup/#configuring-conda","title":"Configuring Conda","text":"<p>Lastly, we will setup the default channels (from where packages will be searched for and downloaded if no channel is specified).</p> <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre>"},{"location":"pages/course-information/pre-course-setup/#installing-snakemake","title":"Installing Snakemake","text":"<p>We will use Conda environments for the set up of this tutorial, but don't worry if you don't understand exactly what everything does - you'll learn all the details at the course. First make sure you're currently situated inside the tutorials directory (<code>workshop-reproducible-research/tutorials</code>) and then create the Conda environment like so:</p> <pre><code>conda env create -f snakemake/environment.yml -n snakemake-env\nconda activate snakemake-env\n</code></pre> <p>Check that Snakemake is installed correctly, for example by executing <code>snakemake --help</code>. This should output a list of available Snakemake settings. If you get <code>bash: snakemake: command not found</code> then you need to go back and ensure that the Conda steps were successful. Once you've successfully completed the above steps you can deactivate your Conda environment using <code>conda deactivate</code> and continue with the setup for the other tools.</p>"},{"location":"pages/course-information/pre-course-setup/#installing-nextflow","title":"Installing Nextflow","text":"<p>We'll use Conda to install Nextflow as well: navigate to <code>workshop-reproducible-research/tutorials</code> and create the Conda environment:</p> <pre><code>conda env create -f nextflow/environment.yml -n nextflow-env\nconda activate nextflow-env\n</code></pre> <p>Check that Nextflow was installed correctly by running <code>nextflow -version</code>. Once you've successfully completed the installation you can deactive the environment using <code>conda deactivate</code> and continue with the other setups, as needed.</p>"},{"location":"pages/course-information/pre-course-setup/#installing-r-markdown","title":"Installing R Markdown","text":"<p>We also use Conda to install R Markdown: make sure your working directory is in the tutorials directory (<code>workshop-reproducible-research/tutorials</code>) and install the necessary R packages defined in the <code>environment.yml</code>:</p> <pre><code>conda env create -f rmarkdown/environment.yml -n rmarkdown-env\n</code></pre> <p>You can then activate the environment followed by running RStudio in the background from the command line:</p> <pre><code>conda activate rmarkdown-env\nrstudio &amp;\n</code></pre> <p>The sluggishness of Conda  Some environments are inherently quite complicated in that they have many and varied dependencies, meaning that the search space for the entire dependency hierarchy becomes huge - leading to slow and sluggish installations. This is often the case for R environments. This can be improved by using Mamba, a faster wrapper around Conda. Simply run <code>conda install -n base mamba</code> to install Mamba in your base environment, and replace any <code>conda</code> command with <code>mamba</code> - except activating and deactivating environments, which still needs to be done using Conda.</p> <p>Once you've successfully completed the above steps you can deactivate your Conda environment using <code>conda deactivate</code> and continue with the setup for the other tools.</p> <p>Windows users  In case you are having trouble installing R and RStudio using Conda, both run well directly on Windows and you may therefore want to install Windows versions of these software for this tutorial (if you haven't done so already). Conda is, however, the recommended way. If you're having issues with graphical applications, please have a look at this website; scroll down to the \"Graphical applications\".</p> <p>RStudio and Conda  In some cases RStudio doesn't play well with Conda due to differing libpaths. The first and simplest thing to try is to always start RStudio from the command line (<code>rstudio &amp;</code>). If you're still having issues, check the available library path by <code>.libPaths()</code> to make sure that it points to a path within your Conda environment. It might be that <code>.libPaths()</code> shows multiple library paths, in which case R packages will be searched for by R in all these locations. This means that your R session will not be completely isolated in your Conda environment and that something that works for you might not work for someone else using the same Conda environment, simply because you had additional packages installed in the second library location. One way to force R to just use the conda library path is to add a <code>.Renviron</code> file to the directory where you start R with these lines:</p> <pre><code>```\nR_LIBS_USER=\"\"\nR_LIBS=\"\"\n```\n</code></pre> <p>... and restart RStudio. The <code>rmarkdown/</code> directory in the course materials already contains this file, so you shouldn't have to add this yourself, but we mention it here for your future projects.</p>"},{"location":"pages/course-information/pre-course-setup/#installing-jupyter","title":"Installing Jupyter","text":"<p>Let's continue using Conda for installing software, since it's so convenient to do so! Move in the tutorials directory (<code>workshop-reproducible-research/tutorials</code>), create a Conda environment from the <code>jupyter/environment.yml</code> file and test the installation of Jupyter, like so:</p> <pre><code>conda env create -f jupyter/environment.yml -n jupyter-env\nconda activate jupyter-env\n</code></pre> <p>Once you've successfully completed the above steps you can deactivate your Conda environment using <code>conda deactivate</code> and continue with the setup for the other tools.</p>"},{"location":"pages/course-information/pre-course-setup/#installing-docker","title":"Installing Docker","text":"<p>Installing Docker is quite straightforward on Mac or Windows and a little more cumbersome on Linux. Note that Docker runs as root, which means that you have to have <code>sudo</code> privileges on your computer in order to install or run Docker. When you have finished installing docker, regardless of which OS you are on, please type <code>docker --version</code> to verify that the installation was successful!</p>"},{"location":"pages/course-information/pre-course-setup/#macos","title":"macOS","text":"<p>Go to docker.com and select download option that is suitable for your computer's architecture (i.e. if you have an Intel chip or a newer Apple M1 chip). This will download a <code>dmg</code> file - click on it when it's done to start the installation. This will open up a window where you can drag the Docker.app to Applications. Close the window and click the Docker app from the Applications menu. Now it's basically just to click \"next\" a couple of times and we should be good to go. You can find the Docker icon in the menu bar in the upper right part of the screen.</p>"},{"location":"pages/course-information/pre-course-setup/#linux","title":"Linux","text":"<p>How to install Docker differs a bit depending on your Linux distribution, but the steps are the same. Please follow the instructions for your distribution on https://docs.docker.com/engine/install/#server.</p> <p>Tip</p> <p>As mentioned before, Docker needs to run as root. You can achieve this by prepending all Docker commands with <code>sudo</code>. This is the approach that we will take in this tutorial, since the set up becomes a little simpler that way. If you plan on continuing using Docker you can get rid of this by adding your user to the group <code>docker</code>. Here are instructions for how to do this: https://docs.docker.com/engine/installation/linux/linux-postinstall/.</p>"},{"location":"pages/course-information/pre-course-setup/#windows","title":"Windows","text":"<p>In order to run Docker on Windows your computer must support Hardware Virtualization Technology and virtualization must be enabled. This is typically done in BIOS. Setting this is outside the scope of this tutorial, so we'll simply go ahead as if though it's enabled and hope that it works.</p> <p>On Windows 10 we will install Docker for Windows, which is available at docker.com. Click the link Download from Docker Hub, and select Get Docker. Once the download is complete, execute the file and follow the instructions. You can now start Docker from the Start menu. You can search for it if you cannot find it; the Docker whale icon should appear in the task bar.</p> <p>You will probably need to enable integration with the Linux subsystem, if you haven't done so during the installation of Docker Desktop. Right-click on the Docker whale icon in the task bar and select Settings. Choose Resources and select WPS integration. Enable integration with the Linux subsystem and click Apply &amp; Restart; also restart the Linux subsystem.</p>"},{"location":"pages/course-information/pre-course-setup/#installing-singularity","title":"Installing Singularity","text":"<p>Installation of Singularity depends, again, on your operating system. When you have finished, regardless of your OS, please type <code>singularity --version</code> to verify that your installation was successful!</p> <p>Both Mac and Windows utilise Vagrant, for which the information in the box below may help you.</p> <p>Vagrant and VirtualBox  The Vagrant VirtualBox with Singularity can be started like this:</p> <ul> <li>Move into the folder <code>vm-singularity</code> where you installed Singularity.</li> <li>Type <code>vagrant up</code> and once this has finished, verify that the Vagrant   VirtualBox is running with <code>vagrant status</code>.</li> <li>Now, type <code>vagrant ssh</code>, which will open the Vagrant VirtualBox.</li> <li>The first time you open the Vagrant VirtualBox like this, you will have to   download the course material to obtain a copy for the Singularity tutorial   within the Vagrant VirtualBox by typing <code>git clone https://github.com/NBISweden/workshop-reproducible-research.git</code>.</li> </ul>"},{"location":"pages/course-information/pre-course-setup/#macos_1","title":"macOS","text":"<p>Please follow the Mac-specific instructions at the Singularity website.</p>"},{"location":"pages/course-information/pre-course-setup/#linux_1","title":"Linux","text":"<p>Follow the Linux-specific instruction at the Singularity website.</p>"},{"location":"pages/course-information/pre-course-setup/#windows_1","title":"Windows","text":"<p>Please follow the Windows-specific instructions at the Singularity website.</p> <p>Notes</p> <p>Last time we checked, the software \"Vagrant Manager\" was not available for download but the installation of Singularity was successful even without it.</p> <p>Version 6.1.28 of \"Virtual box for Windows\" may not work, please install version 6.1.26 from here in case you encounter problems when trying to start the Vagrant VirtualBox.</p>"},{"location":"pages/course-information/pre-course-setup/#testing-sra-tools","title":"Testing sra-tools","text":"<p>On some computers we've found that the package <code>sra-tools</code> which is used in the course is not working properly. The error seems to be related to some certificate used to communicate with remote read archives and may affect all environments with <code>sra-tools</code> on the dependency list.</p> <p>If you run into errors with the program <code>fastq-dump</code> from the <code>sra-tools</code> package try the following:</p> <ol> <li>Remove <code>sra-tools</code> from the relevant environment: <code>conda remove sra-tools</code></li> <li>Download the most recent binaries for your operating system from here (example shown for Mac OSX): <code>curl --output sratoolkit.tar.gz https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-mac64.tar.gz</code></li> <li>Create a temporary directory for the installation: <code>mkdir tmp_out</code></li> <li>Extract the binary files: <code>tar -C tmp_out -zxvf sratoolkit.tar.gz */bin/*</code></li> <li>Copy binary files into the conda environment: <code>cp -r tmp_out/*/bin/* $CONDA_PREFIX/bin/</code></li> <li>Remove the downloaded files: <code>rm -r sratoolkit.tar.gz tmp_out/</code></li> </ol>"},{"location":"pages/course-information/schedule/","title":"Schedule","text":"Day 1   Time   Topic   09:00   Introduction to Next Generation Sequencing (NGS)   10:00   Starting a Metagenomics Project                              - How do you plan a metagenomics experiment?                 - How does a metagenomics project look like?   10:30   Break   10:45   Quick introduction to Galaxy : a powerful bioinformatic tool                              - How to get started in Galaxy ?                 - What is a fastq file ?                 - Introduction to the fastqE tool   12:00   Lunch   13:00   Assessing Read Quality, Trimming and Filtering                              - How can I describe the quality of my data?                 - How can we get rid of sequence data that does not meet our quality standards?   14:30   Break   14:45   Taxonomic Profiling and Visualization of Metagenomic Data                            - Which species (or genera, families, \u2026) are present in my sample?               - What are the different approaches and tools to get the community profile of my sample?               - How can we visualize and compare community profiles?   Day 2   Time   Topic   Teacher   09:00   Calculating \u03b1 and \u03b2 diversity from microbiome taxonomic data                             - How many different taxons are present in my sample? How do I additionally take their relative abundance into account?               - How similar or how dissimilar are my samples in term of taxonomic diversity?               - What are the different metrics used to calculate the taxonomic diversity of my samples?                10:30   Break   10:45   Metagenome Assembly                             - Why should genomic data be assembled?               - What is the difference between reads and contigs?               - How can we assess the quality of an assembly?               - How can we assemble a metagenome?                12:00   Lunch   13:00   Metagenome Binning                 - How can we obtain the original genomes from a metagenome?               - How can we assess the quality of a binning?                14:00   Genome Annotation"},{"location":"pages/galaxy/00-short_intro_galaxy/","title":"Overview","text":"<ul> <li>This is a short introduction to the Galaxy user interface - the web page that you interact with.</li> <li>We will cover key tasks in Galaxy: uploading files, using tools, viewing histories, and running workflows.</li> </ul>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#create-an-account-on-a-galaxy-instanceserver","title":"Create an account on a Galaxy instance/server","text":"<p>If you already have an account, skip to the next section!</p> <p>In Galaxy, server and instance are often used interchangeably. These terms basically mean that different regions have different Galaxy servers/instances, with slightly different tool installations and appearances. If you don't have a specific server/instance in mind, we recommend registering at one of the main public servers/instances, detailed below.</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#what-does-galaxy-look-like","title":"What does Galaxy look like?","text":"<p>Log in to Galaxy</p> <ol> <li>Open your favorite browser (Chrome, Safari, Edge or Firefox, not Internet Explorer!)</li> <li>Browse to your Galaxy instance</li> <li>Log in or register</li> </ol> <p></p> <p>Different Galaxy servers</p> <p>This is an image of Galaxy Australia, located at usegalaxy.org.au</p> <p>The particular Galaxy server that you are using may look slightly different and have a different web address.</p> <p>You can also find more possible Galaxy servers at the top of this tutorial in Available on these Galaxies</p> <p>The Galaxy homepage is divided into four sections (panels): * The Activity Bar on the left: This is where you will navigate to the resources in Galaxy (Tools, Workflows, Histories etc.) * Currently active \"Activity Panel\" on the left: By default, the  Tools activity will be active and its panel will be expanded * Viewing panel in the middle: The main area for context for your analysis * History of analysis and files on the right: Shows your \"current\" history; i.e.: Where any new files for your analysis will be stored</p> <p></p> <p>The first time you use Galaxy, there will be no files in your history panel.</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#key-galaxy-actions","title":"Key Galaxy actions","text":"","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#name-your-current-history","title":"Name your current history","text":"<p>Your \"History\" is in the panel at the right. It is a record of the actions you have taken.</p> <p>Name history</p> <ol> <li>Go to the History panel (on the right)</li> <li> <p>Click \u270f\ufe0f (Edit) next to the history name (which by default is \"Unnamed history\")</p> <p></p> </li> <li> <p>Type in a new name, for example, \"Galaxy Tutorial\"</p> </li> <li>Click Save</li> </ol> <p>Renaming not an option?</p> <p>If renaming does not work, it is possible you aren't logged in, so try logging in to Galaxy first. Anonymous users are only permitted to have one history, and they cannot rename it.</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#upload-a-file","title":"Upload a file","text":"<p>Upload a file from URL</p> <ol> <li> <p>At the top of the Activity Bar, click the  Upload activity</p> <p></p> <p>This brings up a box:</p> </li> <li> <p>Click Choose Local File or Drop the files </p> </li> <li>Paste in the fastq.gz file from the JC1A sample:</li> </ol> <pre><code>JC1A.fastq.gz\n</code></pre> <ol> <li>Click Start</li> <li>Click Close</li> </ol> <p>Your uploaded file is now in your current history. When the file has uploaded to Galaxy, it will turn green.</p> <p>After this you will see your first history item (called a \"dataset\") in Galaxy's right panel. It will go through the gray (preparing/queued) and yellow (running) states to become green (success).</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#what-is-this-file","title":"What is this file?","text":"<p>View dataset contents</p> <ol> <li>Click the \ud83d\udc41\ufe0f (eye) icon next to the dataset name, to look at the file content</li> </ol> <p></p> <p>The contents of the file will be displayed in the central Galaxy panel. If the dataset is large, you will see a warning message which explains that only the first megabyte is shown.</p> <p>This file contains DNA sequencing reads from a bacteria, in FASTQ format:</p> <p></p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#use-a-tool","title":"Use a tool","text":"<p>Let's look at the quality of the reads in this file.</p> <ol> <li>Type FastQE in the tools panel search box (top)</li> <li> <p>Click the tool (FASTQE visualize fastqfiles with emoji's)  The tool will be displayed in the central Galaxy panel.</p> </li> <li> <p>Select the following parameters:</p> <ul> <li>\"Raw read data from your current history\": the FASTQ dataset that we uploaded (should be already selected)</li> <li>No change in the other parameters</li> </ul> </li> <li>Click Run Tool</li> </ol> <p>This tool will run and two one new output datasets will appear at the top of your history panel. </p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#view-results","title":"View results","text":"<p>We will now look at the output dataset called FastQE on data 1.</p> <p>Comment</p> <ul> <li>Note that Galaxy has given this dataset a name according to both the tool name (\"FastQE\") and the input (\"data 1\") that it used.</li> <li>The name \"data 1\" means the dataset number 1 in Galaxy's current history (our FASTQ file).</li> </ul> <p>View results</p> <ul> <li>Once it's green, click the \ud83d\udc41\ufe0f (eye) icon next to the \"Webpage\" output dataset.</li> </ul> <p>The information is displayed in the central panel</p> <p>!</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#share-your-history","title":"Share your history","text":"<p>Share your history</p> <p>Imagine you had a problem in your analysis and want to ask for help. Try to create a link for your history and share it with yourself.</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#convert-your-analysis-history-into-a-workflow","title":"Convert your analysis history into a workflow","text":"<p>Galaxy records every tool you run and the parameters used. You can convert this history into a workflow to reuse later.</p> <p>Extract workflow</p> <ol> <li>Clean up your history: remove any failed (red) jobs</li> <li>Click (History options) \u2192 Extract workflow</li> <li>Select the steps to include</li> <li>Replace the workflow name (e.g., <code>QC and filtering</code>)</li> <li>Rename the workflow input (e.g., <code>FASTQ reads</code>)</li> <li>Click Create Workflow</li> </ol>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#create-a-new-history","title":"Create a new history","text":"<p>New history</p> <ol> <li>Create a new history</li> <li>Rename it, e.g., \"Next Analysis\"</li> </ol> <p>This new history has no datasets yet.</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#look-at-multiple-histories","title":"Look at multiple histories","text":"<p>View histories in History Multiview</p> <ol> <li>Open History Multiview in the activity bar</li> <li>Or click Show Histories side-by-side</li> <li>Copy a dataset into your new history by dragging it from \"My Analysis\" to \"Next Analysis\"</li> <li>Return to the main Galaxy window</li> </ol> <p>Comment</p> <p>This is not the only way to view your histories in Galaxy: 1. An exhaustive list is available in the My Histories tab 2. You can quickly switch to another history using History options</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#run-workflow-in-the-new-history","title":"Run workflow in the new history","text":"<p>Run workflow</p> <ol> <li>Click the Workflows activity in the activity bar</li> <li>Select your newly created workflow</li> <li>Configure the input (e.g., select the FASTQ dataset)</li> <li>Click Run Workflow</li> </ol> <p>You should see a message that the workflow was successfully invoked and jobs will start running.</p>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/00-short_intro_galaxy/#conclusion","title":"Conclusion","text":"<p>Well done! You have completed the short introduction to Galaxy:</p> <ul> <li>Named a history</li> <li>Uploaded a file</li> <li>Used a tool</li> <li>Viewed results</li> <li>Ran a workflow</li> </ul>","tags":["espa\u00f1ol"]},{"location":"pages/galaxy/02-taxonomy-galaxy/","title":"02 taxonomy galaxy","text":"<p>The term \"microbiome\" describes \"a characteristic microbial community occupying a reasonably well-defined habitat which has distinct physio-chemical properties. The term thus not only refers to the microorganisms involved but also encompasses their theatre of activity\" ((whipps1988mycoparasitism )).</p> <p>Microbiome data can be gathered from different environments such as soil, water or the human gut. The biological interest lies in general in the question how the microbiome present at a specific site influences this environment. To study a microbiome, we need to use indirect methods like metagenomics or metatranscriptomics.</p> <p>Metagenomic samples contain DNA from different organisms at a specific site, where the sample was collected. Metagenomic data can be used to find out which organisms coexist in that niche and which genes are present in the different organisms. Metatranscriptomic samples include the transcribed gene products, thus RNA, that therefore allow to not only study the presence of genes but additionally their expression in the given environment. The following tutorial will focus on metagenomics data, but the principle is the same for metatranscriptomics data.</p> <p>The investigation of microorganisms present at a specific site and their relative abundance is also called \"microbial community profiling\". The main objective is to identify the microorganisms that are present within the given sample. This can be achieved for all known microbes, where the DNA sequence specific for a certain species is known.</p> <p>For that we try to identify the taxon to which each individual read belongs.</p> <p>For metagenomic data analysis we start with sequences derived from DNA fragments that are isolated from the sample of interest. Ideally, the sequences from all microbes in the sample are present. The underlying idea of taxonomic assignment is to compare the DNA sequences found in the sample (reads) to DNA sequences of a database. When a read matches a database DNA sequence of a known microbe, we can derive a list with microbes present in the sample.</p> <p>When talking about taxonomic assignment or taxonomic classification, most of the time we actually talk about two methods, that in practice are often used interchangeably: - taxonomic binning: the clustering of individual sequence reads based on similarities criteria and assignation of clusters to reference taxa - taxonomic profiling: classification of individual reads to reference taxa to extract the relative abundances of the different taxa</p> <p>The reads can be obtained from amplicon sequencing  (e.g. 16S, 18S, ITS), where only specific gene or gene fragments are targeted (using specific primers) or shotgun metagenomic sequencing, where all the accessible DNA of a mixed community is amplified (using random primers). This tutorial focuses on reads obtained from shotgun metagenomic sequencing.</p>"},{"location":"pages/galaxy/02-taxonomy-galaxy/#taxonomic-profiling","title":"Taxonomic profiling","text":"<p>Tools for taxonomic profiling can be divided into three groups. Nevertheless, all of them require a pre-computed database based on previously sequenced microbial DNA or protein sequences. 1. DNA-to-DNA: comparison of sequencing reads with genomic databases of DNA sequences with tools like Kraken ((Wood2014 )) 2. DNA-to-Protein : comparison of sequencing reads with protein databases (more computationally intensive because all six frames of potential DNA-to amino acid translations need to be analyzed) with tools like DIAMOND) 3. Marker based: searching for marker genes (e.g. 16S rRNA sequence) in reads, which is quick, but introduces bias, with tools like MetaPhlAn ((blanco2023extending ))</p> <p>The comparison of reads to database sequences can be done in different ways, leading to three different types of taxonomic assignment:</p> <ul> <li>Genome based approach</li> </ul> <p>Reads are aligned to reference genomes. Considering the coverage and breadth, genomes are used to measure genome abundance. Furthermore, genes can be analyzed in genomic context. Advantages of this method are the high detection accuracy, that the unclassified percentage is known, that all SNVs can be detected and that high-resolution genomic comparisons are possible. This method takes medium compute cost.</p> <ul> <li>Gene based approach</li> </ul> <p>Reads are aligned to reference genes. Next, marker genes are used to estimate species abundance. Furthermore, genes can be analyzed in isolation for presence or absence in a specific condition. The major advantage is the detection of the pangenome (entire set of genes within a species). Major disadvantages are the high compute cost, low detection accuracy and that the unclassified percentage is unknown. At least intragenic SNVs can be detected and low-resolution genomic comparison is possible.</p> <ul> <li>k-mer based approach</li> </ul> <p>Databases as well as the samples DNA are broken into strings of length  for comparison. From all the genomes in the database, where a specific k-mer is found, a lowest common ancestor (LCA) tree is derived and the abundance of k-mers within the tree is counted. This is the basis for a root-to-leaf path calculation, where the path with the highest score is used for classification of the sample. By counting the abundance of k-mers, also an estimation of relative abundance of taxa is possible. The major advantage of k-mer based analysis is the low compute cost. Major disadvantages are the low detection accuracy, that the unclassified percentage is unknown and that there is no gene detection, no SNVs detection and no genomic comparison possible. An example for a k-mer based analysis tool is Kraken, which will be used in this tutorial.</p> <p>After this theoretical introduction, let's now get hands on analyzing an actual dataset!</p>"},{"location":"pages/galaxy/02-taxonomy-galaxy/#background-on-data","title":"Background on data","text":"<p>The dataset we will use for this tutorial comes from an oasis in the Mexican desert called Cuatro Ci\u00e9negas ((Okie.2020 )). The researchers were interested in genomic traits that affect the rates and costs of biochemical information processing within cells. They performed a whole-ecosystem experiment, thus fertilizing the pond to achieve nutrient enriched conditions.</p> <p>Here we will use 2 datasets: - <code>JP4D</code>: a microbiome sample collected from the Lagunita Fertilized Pond - <code>JC1A</code>: a control samples from a control mesocosm.</p> <p>The datasets differ in size, but according to the authors this doesn't matter for their analysis of genomic traits. Also, they underline that differences between the two samples reflect trait-mediated ecological dynamics instead of microevolutionary changes as the duration of the experiment was only 32 days. This means that depending on available nutrients, specific lineages within the pond grow more successfully than others because of their genomic traits.</p> <p>The datafiles are named according to the first four characters of the filenames. It is a collection of paired-end data with R1 being the forward reads and R2 being the reverse reads. Additionally, the reads have been trimmed using cutadapt as explained in the Quality control tutorial.</p> <p>Agenda</p> <p>In this tutorial, we will cover:</p> <ol> <li>TOC</li> </ol> <p>Info</p>"},{"location":"pages/galaxy/02-taxonomy-galaxy/#prepare-galaxy-and-data","title":"Prepare Galaxy and data","text":"<p>Any analysis should get its own Galaxy history. So let's start by creating a new one:</p> <p>Hands-on: Data upload</p> <ol> <li> <p>Create a new history for this analysis</p> </li> <li> <p>Rename the history</p> </li> </ol> <p>Example</p> <p>Now, we need to import the data</p> <p>Hands-on: Import datasets</p> <ol> <li>Import the following samples via link from Zenodo or Galaxy shared data libraries:</li> </ol> <pre><code>{{ no such element: mkdocs.structure.pages.Page object['zenodo_link'] }}/files/JC1A_R1.fastqsanger.gz\n{{ no such element: mkdocs.structure.pages.Page object['zenodo_link'] }}/files/JC1A_R2.fastqsanger.gz\n{{ no such element: mkdocs.structure.pages.Page object['zenodo_link'] }}/files/JP4D_R1.fastqsanger.gz\n{{ no such element: mkdocs.structure.pages.Page object['zenodo_link'] }}/files/JP4D_R2.fastqsanger.gz\n</code></pre> <ol> <li>Create a paired collection.</li> </ol> <p>Example</p>"},{"location":"pages/galaxy/02-taxonomy-galaxy/#k-mer-based-taxonomic-assignment-with-kraken2","title":"k-mer based taxonomic assignment with Kraken2","text":"<p>Our input data is the DNA reads of microbes present at Cuatro Ci\u00e9negas.</p> <p>To find out which microorganisms are present, we will compare the reads of the sample to a reference database, i.e. sequences of known microorganisms stored in a database, using Kraken2 ((wood2019improved )).</p> <p>For this tutorial, we will use the PlusPF database which contains the Standard (archaea, bacteria, viral, plasmid, human, UniVec_Core), protozoa and fungi data.</p> <p>Details: What is in the PlusPF database</p> Database Origin Archaea RefSeq complete archaeal genomes/proteins Bacteria RefSeq complete bacterial genomes/proteins Plasmid RefSeq plasmid nucleotide/protein sequences Viral RefSeq complete viral genomes/proteins Human GRCh38 human genome/proteins Fungi RefSeq complete fungal genomes/proteins Plant RefSeq complete plant genomes/proteins Protozoa RefSeq complete protozoan genomes/proteins UniVec_Core A subset of UniVec, NCBI-supplied database of vector, adapter, linker, and primer sequences that may be contaminating sequencing projects and/or assemblies, chosen to minimize false positive hits to the vector database <p>The databases have been prepared for Kraken2 by Ben Leagmead and details can found on his page.</p> Note <p>Hands-on: Assign taxonomic labels with Kraken2</p> <ol> <li>Kraken2 \u2013 Tool link with the following parameters:</li> <li>\"Single or paired reads\": <code>Paired Collection</code><ul> <li>\"Collection of paired reads\": Input paired collection</li> </ul> </li> <li> <p>\"Confidence\": <code>0.1</code></p> <p>A confidence score of 0.1 means that at least 10% of the k-mers should match entries in the database. This value can be reduced if a less restrictive taxonomic assignation is desired.</p> </li> <li> <p>In \"Create Report\":</p> <ul> <li>\"Print a report with aggregrate counts/clade to file\": <code>Yes</code></li> </ul> </li> <li>\"Select a Kraken2 database\": most recent <code>Prebuilt Refseq indexes: PlusPF</code> </li> </ol> <p>Example</p> <p>Kraken2 will create two outputs for each dataset</p> <ul> <li> <p>Classification: tabular files with one line for each sequence classified by Kraken and 5 columns:</p> </li> <li> <p><code>C</code>/<code>U</code>: a one letter indicating if the sequence classified or unclassified</p> </li> <li>Sequence ID as in the input file</li> <li>NCBI taxonomy ID assigned to the sequence, or 0 if unclassified</li> <li>Length of sequence in bp (<code>read1|read2</code> for paired reads)</li> <li> <p>A space-delimited list indicating the lowest common ancestor (LCA) mapping of each k-mer in the sequence</p> <p>For example, <code>562:13 561:4 A:31 0:1 562:3</code> would indicate that:   1. The first 13 k-mers mapped to taxonomy ID #562   2. The next 4 k-mers mapped to taxonomy ID #561   3. The next 31 k-mers contained an ambiguous nucleotide   4. The next k-mer was not in the database   5. The last 3 k-mers mapped to taxonomy ID #562</p> <p><code>|:|</code> indicates end of first read, start of second read for paired reads</p> </li> </ul> <p>For JC1A:</p> <pre><code>Column 1 Column 2    Column 3    Column 4    Column 5\nU    MISEQ-LAB244-W7:91:000000000-A5C7L:1:1101:13417:1998    0   151|190     A:17 0:15 2055:5 0:1 2220095:5 0:74 |:| 0:3 A:53 2:2 0:32 204455:1 2823043:5 0:60\nU    MISEQ-LAB244-W7:91:000000000-A5C7L:1:1101:15782:2187    0   169|173     0:101 37329:1 0:33 |:| 0:10 2751189:5 0:30 1883:2 0:39 2609255:5 0:48\nU    MISEQ-LAB244-W7:91:000000000-A5C7L:1:1101:11745:2196    0   235|214     0:173 2282523:5 2746321:2 0:21 |:| 0:65 2746321:2 2282523:5 0:108\nU    MISEQ-LAB244-W7:91:000000000-A5C7L:1:1101:18358:2213    0   251|251     0:35 281093:5 0:3 651822:5 0:145 106591:3 0:21 |:| 0:64 106591:3 0:145 651822:5\nU    MISEQ-LAB244-W7:91:000000000-A5C7L:1:1101:14892:2226    0   68|59   0:34 |:| 0:25\nU    MISEQ-LAB244-W7:91:000000000-A5C7L:1:1101:18764:2247    0   146|146     0:112 |:| 0:112\nC    MISEQ-LAB244-W7:91:000000000-A5C7L:1:1101:12147:2252    9606    220|220     9606:148 0:19 9606:19 |:| 9606:19 0:19 9606:148\n</code></pre> <p>Question</p> <p>For JC1A sample 1. Is the first sequence in the file classified or unclassified? 2. What is the taxonomy ID assigned to the first classified sequence? 3. What is the corresponding taxon?</p> <p>Solution 1. unclassified 2. 9606, for the line 7 3. 9606 corresponds to Homo sapiens when looking at NCBI.</p> Success <p>??? question</p> <ul> <li> <p>Report: tabular files with one line per taxon and 6 columns or fields</p> </li> <li> <p>Percentage of fragments covered by the clade rooted at this taxon</p> </li> <li>Number of fragments covered by the clade rooted at this taxon</li> <li>Number of fragments assigned directly to this taxon</li> <li> <p>A rank code, indicating</p> <ul> <li>(U)nclassified</li> <li>(R)oot</li> <li>(D)omain</li> <li>(K)ingdom</li> <li>(P)hylum</li> <li>(C)lass</li> <li>(O)rder</li> <li>(F)amily</li> <li>(G)enus, or</li> <li>(S)pecies</li> </ul> <p>Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., <code>G2</code> is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank.</p> </li> <li> <p>NCBI taxonomic ID number</p> </li> <li>Indented scientific name</li> </ul> <pre><code>Column 1 Column 2    Column 3    Column 4    Column 5    Column 6\n76.85    105397  105397  U   0   unclassified\n23.15    31742   1197    R   1   root\n22.20    30450   312     R1  131567  cellular organisms\n12.58    17256   3768    D   2   Bacteria\n8.77     12028   2868    P   1224    Proteobacteria\n4.94     6779    3494    C   28211   Alphaproteobacteria\n1.30     1782    1085    O   204455  Rhodobacterales\n0.43     593     461     F   31989   Rhodobacteraceae\n0.05     74  53  G   265     Paracoccus\n</code></pre> <p>Question</p> <ol> <li>What are the percentage on unclassified for JC1A and JP4D?</li> <li>What are the kindgoms found for JC1A and JP4D?</li> <li>Where might the eukaryotic DNA come from?</li> <li>How is the diversity of Proteobacteria in JC1A and JP4D?</li> </ol> <p>Solution</p> <ol> <li>77% for JC1A and 90% for JP4D</li> <li>Kindgoms:</li> <li>JC1A: 13% Bacteria, 9% Eukaryota, 0.03% Virus</li> <li>JP4D: 9% Bacteria, 0.7% Eukaryota</li> <li>It seems to be human contamination</li> <li>JC1A seems to have a big diversity of classes and species of Proteobacteria. JP4D seems more dominated by Aphaproteobacteria.</li> </ol> Success <p>??? question</p> <p>Getting an overview of the assignation is not straightforward with the Kraken2 outputs directly. We can use visualisation tools for that.</p> <p>A \"simple and worthwile addition to Kraken for better abundance estimates\" ((Ye.2019 )) is called Bracken (Bayesian Reestimation of Abundance after Classification with Kraken). Instead of only using proportions of classified reads, it takes a probabilistic approach to generate final abundance profiles. It works by re-distributing reads in the taxonomic tree: \"Reads assigned to nodes above the species level are distributed down to the species nodes, while reads assigned at the strain level are re-distributed upward to their parent species\" ((Lu.2017 )).</p> <p>Hands-on: Estimate species abundance with Bracken</p> <ol> <li> <p>Estimate Abundance at Taxonomic Level \u2013 Tool link with the following parameters:</p> <ul> <li>\"Kraken report file\": Report output of Kraken</li> <li>\"Select a kmer distribution\": <code>PlusPF</code>, same as for Kraken</li> </ul> <p>It is important to choose the same database that you also chose for Kraken2</p> <ul> <li>\"Level\": <code>Species</code></li> <li>\"Produce Kraken-Style Bracken report\": <code>yes</code> </li> </ul> </li> </ol> <p>Example</p>"},{"location":"pages/galaxy/02-taxonomy-galaxy/#visualization-of-taxonomic-assignment","title":"Visualization of taxonomic assignment","text":"<p>Once we have assigned the corresponding taxa to each sequence, the next step is to properly visualize the data. There are several tools for that: - Krona ((Ondov.2011 )) - Phinch ((Bik.2014 )) - Pavian ((Breitwieser.2020 ))</p>"},{"location":"pages/galaxy/02-taxonomy-galaxy/#visualisation-using-krona","title":"Visualisation using Krona","text":"<p>Krona creates an interactive HTML file allowing hierarchical data to be explored with zooming, multi-layered pie charts. With this tool, we can easily visualize the composition of the bacterial communities and compare how the populations of microorganisms are modified according to the conditions of the environment.</p> <p>Kraken outputs can not be given directly to Krona, they first need to be converted.</p> <p>Krakentools ((Lu.2017 )) is a suite of tools to work on Kraken outputs. It include a tool designed to translate results of the Kraken metagenomic classifier to the full representation of NCBI taxonomy. The output of this tool can be directly visualized by the Krona tool.</p> <p>Hands-on: Convert Kraken report file</p> <ol> <li>Krakentools: Convert kraken report file \u2013 Tool link with the following parameters:</li> <li> <p>\"Kraken report file\": Report collection of Kraken</p> </li> <li> <p>Inspect the generated output for JC1A</p> </li> </ol> <p>Example</p> <p>Question</p> <pre><code>3869  k__Bacteria\n2868  k__Bacteria     p__Proteobacteria\n3494  k__Bacteria     p__Proteobacteria   c__Alphaproteobacteria\n1085  k__Bacteria     p__Proteobacteria   c__Alphaproteobacteria  o__Rhodobacterales\n461   k__Bacteria     p__Proteobacteria   c__Alphaproteobacteria  o__Rhodobacterales  f__Rhodobacteraceae\n53    k__Bacteria     p__Proteobacteria   c__Alphaproteobacteria  o__Rhodobacterales  f__Rhodobacteraceae     g__Paracoccus\n10    k__Bacteria     p__Proteobacteria   c__Alphaproteobacteria  o__Rhodobacterales  f__Rhodobacteraceae     g__Paracoccus   s__Paracoccus_pantotrophus\n6     k__Bacteria     p__Proteobacteria   c__Alphaproteobacteria  o__Rhodobacterales  f__Rhodobacteraceae     g__Paracoccus   s__Paracoccus_sanguinis\n4     k__Bacteria     p__Proteobacteria   c__Alphaproteobacteria  o__Rhodobacterales  f__Rhodobacteraceae     g__Paracoccus   s__Paracoccus_sp._AK26\n1     k__Bacteria     p__Proteobacteria   c__Alphaproteobacteria  o__Rhodobacterales  f__Rhodobacteraceae     g__Paracoccus   s__Paracoccus_sp._MA\n</code></pre> <ol> <li>What are the different columns?</li> <li>What are the lines?</li> </ol> <p>Solution</p> <ol> <li>Column 1 seems to correspond to the number of fragments covered by a taxon, the columns after represent the different taxonomic level (from kingdom to species)</li> <li>A line is a taxon with its hierarchy and the number of reads assigned to it</li> </ol> Success Question <p>Let's now run Krona</p> <p>Hands-on: Generate Krona visualisation 1. Krona pie chart \u2013 Tool link with the following parameters:    - \"Type of input data\": <code>Tabular</code>    -  \"Input file\": output of Krakentools</p> <ol> <li>Inspect the generated file</li> </ol> <p>Example</p> <p></p> <p>Question</p> <ol> <li>What are the percentage on unclassified for JC1A and JP4D?</li> <li>What are the kindgoms found for JC1A and JP4D?</li> <li>Where might the eukaryotic DNA come from?</li> <li>How is the diversity of Proteobacteria in JC1A and JP4D?</li> </ol> <p>Solution</p> <ol> <li>78% for JC1A and 90% for JP4D</li> <li>Kindgoms:</li> <li>JC1A: 13% Bacteria, 10% Eukaryota, 0.03% Virus</li> <li>JP4D: 10% Bacteria, 0.7% Eukaryota</li> <li>It seems to be human contamination</li> <li>JC1A seems to have a big diversity of classes and species of Proteobacteria. JP4D seems more dominated by Aphaproteobacteria.</li> </ol> Success Question"},{"location":"pages/galaxy/02-taxonomy-galaxy/#visualization-using-pavian","title":"Visualization using Pavian","text":"<p>Pavian (pathogen visualization and more) ((Breitwieser.2020 )) is an interactive visualization tool for metagenomic data. It was developed for the clinical metagenomic problem to find a disease-causing pathogen in a patient sample, but it is useful to analyze and visualize any kind of metagenomics data.</p> <p>Hands-on: Launch Pavian 1. Pavian \u2013 Tool link with the following paramters:     -  \"Kraken and MetaPhlAn-style reports\": Report collection of Kraken </p> <p>Example</p> <p>Pavian runs a Galaxy Interactive tool. You can access it when it become orange.</p> <p>Hands-on: Interact with Pavian</p> <ol> <li> <p>Open Pavian</p> </li> <li> <p>Import data</p> </li> <li>Click on <code>Use data on server</code></li> <li>Select both samples</li> <li>Click on <code>Read selected directories</code></li> <li> <p>Check you have a table in <code>Available sample sets</code> looks like</p> X FormatOK Include Name ReportFile ReportFilePath 1 X X JP4AD JP4AD /home/shiny//JP4AD 2 X X JC1A JC1A /home/shiny//JC1A </li> <li> <p>Click on <code>Save table</code></p> </li> <li> <p>Click on <code>Results Overview</code> in the left panel </p> </li> </ol> <p>Example</p> <p>This page shows the summary of the classifications in the selected sample set:</p> <p></p> <p>Question</p> <ol> <li>Does both sample have same size?</li> <li>What are the percentage of classified reads for JC1A and JP4D?</li> <li>Are the percentage of bacterial reads similar?</li> </ol> <p>Solution</p> <ol> <li>JP4D has much more reads than JC1A</li> <li>10.2% for JP4D and 23.1% for JC1A</li> <li>12.6% for JC1A and 9.22% for JP4D. So similar magnitude orders</li> </ol> Success Question <p>Let's now inspect assignements to reads per sample.</p> <p>Hands-on: Inspect samples with Pavian</p> <ol> <li>Click on <code>Sample</code> in the left panel</li> <li>Select <code>JC1A</code> in the <code>Select sample</code> drop-down on the top</li> </ol> <p>The first view gives a Sankey diagram for one sample:</p> <p></p> <p>Question</p> <ol> <li>What is a Sankey diagram?</li> <li>What are the different set of values represented as the horizontal axis?</li> </ol> <p>Solution</p> <ol> <li>A sankey diagram is a visualization used to depict a flow from one set of values to another</li> <li>The taxonomy hierarchy from domain on the left to species on the right</li> </ol> Success <p>??? question</p> <ol> <li>Click on <code>Proteobacteria</code> in the Sankey plot</li> <li>Inspect the created graph on the right</li> </ol> <p></p> <p>Question</p> <ol> <li>Are the number of reads assigned to Proteobacteria similar for both samples?</li> <li>Why?</li> </ol> <p>Solution</p> <ol> <li>JP4A has many more reads assigned to Proteobacteria than JC1A</li> <li>JP4A has many more reads initialls</li> </ol> Success <p>??? question</p> <p>Example</p> <p>We would like now to compare both samples.</p> <p>Hands-on: Inspect samples with Pavian</p> <ol> <li>Click on <code>Comparison</code> in the left panel</li> <li>Select <code>%</code>  and unclick <code>Reads</code> in the blue area drop-down on the top</li> <li>Click on <code>Domain</code> green button</li> </ol> <p></p> <p>Question</p> <p>Is there similar proportion of Bacteria in both samples?</p> <p>Solution</p> <p>JP4D has much higher proportion of Bacteria (&gt; 93%&gt;) than JC1A (57%), which contains quite a lot of Eukaryote</p> Success <p>??? question</p> <ol> <li>Select <code>Homo sapiens</code> in the <code>Filter taxa</code> box below the green buttons</li> </ol> <p>Question</p> <p>Is there similar proportion of Bacteria in both samples?</p> <p>Solution</p> <p>After human filtering, both samples have similar proportion of Bacteria</p> Success <p>??? question</p> <ol> <li>Click on <code>Class</code> green button</li> </ol> <p>Question</p> <ol> <li>How are the diversities of classes in both samples?</li> <li>What could it biologically mean given that JC1A is a control and JP4D a sample from fertilized pond?</li> </ol> <p>Solution</p> <ol> <li>JP4D seems highly dominated by a Alphaproteobacteria class. JC1A has also a majority of Alphaproteobacteria, but also significant proportions of Betaproteobacteria, Gammaproteobacteria, Flavobacteria, Actinomycetia</li> <li>Alphaproteobacteria seems to have a survival advantage in the new environment. According to the authors this correlates with specific genomic traits that enable them to cope better with high nutrient availability.</li> </ol> Success <p>??? question </p> <p>Example</p> <p>Once you are done with Pavian, you should delete it in your history so the corresponding job is killed.</p> <p>Details: Visualize the taxonomical classification with Phinch</p> <p>Phinch ((Bik.2014 )) is another tools to visualize large biological datasets like our taxonomic classification. Taxonomy Bar Charts, Bubble Charts, Sankey Diagrams, Donut Partitions and Attributes Column Chart can be generated using this tool.</p> <p>As a first step, we need to convert the Kraken output file into a kraken-biom file to make it accessible for Phinch. For this, we need to add a metadata file, provided on Zenodo. When generating a metadata file for your own data, you can take this as an example and apply the general guidelines.</p> <p>Hands-on: Phinch 1. Import the metadata tabular from Zenodo or Galaxy shared data libraries:</p> <pre><code>{{ no such element: mkdocs.structure.pages.Page object['zenodo_link'] }}/files/metadata.tabular\n</code></pre> <ol> <li>Use Kraken-biom \u2013 Tool link to convert Kraken2 report into the correct format for phinch with the following parameters.</li> <li>\"Input\": Report output of Kraken2</li> <li>\"Sample Metadata file\": Metadata tabular</li> <li> <p>\"Output format\": <code>JSON</code></p> </li> <li> <p>Phinch Visualisation \u2013 Tool link with the following paramters:</p> </li> <li>\"Input\": <code>Kraken-biom output file</code></li> </ol> <p>Example</p> <p>Phinch runs a Galaxy Interactive tool. You can access it when it become orange.</p> <p>Hands-on: Interact with Phinch</p> <ol> <li>Open Phinch</li> </ol> <p>The first pages shows an overview of your samples. Here, you have the possibility to further filter your data, for example by date or location, depending on which information you provided in your metadata file.</p> <p>Question</p> <ol> <li>How many sequence reads do the samples contain?</li> </ol> <p>Solution</p> <ol> <li>JC1A (Phinch name: 0) contains 56008 reads, while JP4D (Phinch name: 1) contains 242438 reads</li> </ol> Success <p>??? question</p> <ol> <li>Click on <code>Proceed to gallery</code> to see an overview of all visualization options.</li> </ol> <p>Example</p> <p>Let's have a look at the taxonomy bar chart. Here, you can see the abundance of different taxa depicted in different colors in your samples. On top of the chart you can select which rank is supposed to be shown in the chart.  You can also change the display options to for example switch between value und percentage.</p> <p>Question</p> <ol> <li>What information can you get from hovering over a sample?</li> <li>How many percent of the sample reads are bacteria and how many are eukaryota?</li> </ol> <p>Solution</p> <ol> <li>the taxon\u2019s name and the taxonomy occurrence in the sample</li> <li>choose kingdom and hover over the bars to find \"taxonomy occurence in this sample\":</li> <li>Sample 0: 75,65 % bacteria; 24,51 % eukaryota</li> <li>Sample 1: 92,70 % bacteria; 6,87 % eukaryota</li> </ol> Success Question <p>Let's go back to the gallery and choose the bubble chart. Here, you can find the distribution of taxa across the whole dataset at the rank that you can choose above the chart. When hovering over the bubbles, you get additional information concerning the taxon.</p> <p>Question</p> <ol> <li>Which is the most abundant Class?</li> <li>How many reads are found in both samples?</li> </ol> <p>Solution</p> <p>To order the bubbles according to their size you can choose the <code>list</code> option shown right next to the taxonomy level. Clicking on one bubble gives you the direct comparison of the distribution of this taxon in the different samples. 1. The most abundant Class is Alphaproteobacteria 2. With 18.114 reads in Sample 0 and 153.230 reads in sample 1</p> Success Question <p>Another displaying option is the Sankey diagram, that is depicting the abundance of taxonomies as a flow chart. Again, you can choose the taxonomy level that you want to show in your diagram. When clicking on one bar of the diagram, this part is enlarged for better view.</p> <p>The donut partition summarizes the microbial community according to non-numerical attributes. In the drop-down menu at the top right corner, you can switch between the different attributes provided in the metadata file. In our case, you can for example choose the 'environmental medium' to see the difference between sediment and water (It doesn't really make a lot of sense in our very simple dataset, as this will show the same result as sorting them by sample 0 and 1, but if attributes change across different samples this might be an interesting visualization option). When clicking on one part of the donut you will also find the distribution of the taxon across the samples. On the right hand side you can additionally choose if you\u2019d like to have dynamic y axis or prefer standard y axis to compare different donuts with each other.</p> <p>The attributes column chart summarizes the microbial community according to numerical attributes. In the drop-down menu at the top right corner, you can switch between the different attributes provided in the metadata file. In our case, you can for example choose the 'geographic location' to (again, it doesn't really make a lot of sense in our very simple dataset, as this will show the same result as sorting them by sample 0 and 1, but if attributes change across different samples this might be an interesting visualization option).</p> <p>Once you are done with Phinch, you should delete it in your history so the corresponding job is killed.</p> Note"},{"location":"pages/galaxy/02-taxonomy-galaxy/#choosing-the-right-tool","title":"Choosing the right tool","text":"<p>When it comes to taxonomic assignment while analyzing metagenomic data, Kraken2 is not the only tool available. Several papers do benchmarking of different tools ((Meyer.2022 ),(Sczyrba.2017 ),(Ye.2019 )).</p> <p>Details: Benchmarking taxonomic classification tools</p> <p>The benchmarking papers present different methods for comparing the available tools: - The CAMI challenge is based on results of different labs that each used the CAMI dataset to perform their analysis on and send it back to the authors. - (Ye.2019 ) performed all the analysis themselves.</p> <p>Additionally, the datasets used for both benchmarking approaches differ: - CAMI: only ~30%-40% of reads are simulated from known taxa while the rest of the reads are from novel taxa, plasmids or simulated evolved strains. - (Ye.2019 ) used International Metagenomics and Microbiome Standards Alliance (IMMSA) datasets, wherein the taxa are described better.</p> <p>When benchmarking different classification tools, several metrics are used to compare their performance: 1. Precision: proportion of true positive species identified in the sample divided by number of total species identified by the method. 2. Recall: proportion of true positive species divided by the number of distinct species actually in the sample. 3. Precision-recall curve: each point represents the precision and recall scores at a specific abundance threshold, the area under the precision-recall curve (AUPR) 4. L2 distance: representation of abundance profiles \u2192 how accurately the abundance of each species or genera in the resulting classification reflects the abundance of each species in the original biological sample (\u201cground truth\u201d)</p> <p>When it comes to taxonomic profiling, thus investigating the abundance of specific taxa, the biggest problem is the abundance bias. It is introduced during isolation of DNA (which might work for some organisms better then for others) and by PCR duplicates during PCR amplification.</p> Note <p>Details: Profiling tools</p> <p>Profilers, which are tools that investigate relative abundances of taxa within a dataset, fall into three groups depending on their performance: 1. Profilers, that correctly predict relative abundances 2. Precise profilers (suitable, when many false positives would increase cost and effort in downstream analysis) 3. Profilers with high recall (suitable for pathogen detection, when the failure of detecting an organism can have severe negative consequences)</p> <p>However, some characteristics are common to all profilers: - Most profilers only perform well until the family level - Drastic decrease in performance between family and genus level, while little change between order and family level - Poorer performance of all profilers on CAMI datasets compared to International Metagenomics and Microbiome Standards Alliance (IMMSA) - Fidelity of abundance estimates decreases notably when viruses and plasmids were present - High numbers of false positive calls at low abundance - Taxonomic profilers vs profiles from taxonomic binning: Precision and recall of the taxonomic binners were comparable to that of the profilers; abundance estimation at higher ranks was more problematic for the binners</p> Note Tool Approach Available in Galaxy CAMI challenge (Ye.2019 ) mOTUs No Most memory efficient MetaPhlAn Marker genes Yes Recommended for low computational requirements (&lt; 2 Gb of memory) DUDes No FOCUS No Fast, most memory efficient Bracken k-mer Yes Fast Kraken k-mer Yes Fastest; most memory efficient Good performance metrics; very fast on large numbers of samples; allow custom databases when high amounts of memory (&gt;100 Gb) are available"},{"location":"pages/galaxy/02-taxonomy-galaxy/#marker-gene-based-approach-using-metaphlan","title":"Marker gene based approach using MetaPhlAn","text":"<p>In this tutorial, we follow second approach using MetaPhlAn ((truong2015metaphlan2 )). MetaPhlAn is a computational tool for profiling the composition of microbial communities (Bacteria, Archaea and Eukaryotes) from metagenomic shotgun sequencing data (i.e. not 16S) at species-level. MetaPhlAn 4 relies on ~5.1M unique clade-specific marker genes identified from ~1M microbial genomes (~236,600 references (bacterial, archeal, viral and eukaryotic) and 771,500 metagenomic assembled genomes) spanning 26,970 species-level genome bins.</p> <p>It allows: - unambiguous taxonomic assignments; - accurate estimation of organismal relative abundance; - species-level resolution for bacteria, archaea, eukaryotes and viruses; - strain identification and tracking - orders of magnitude speedups compared to existing methods. - microbiota strain-level population genomics - MetaPhlAn clade-abundance estimation</p> <p>The basic usage of MetaPhlAn consists in the identification of the clades (from phyla to species and strains in particular cases) present in the microbiota obtained from a microbiome sample and their relative abundance.</p> <p>MetaPhlAn in Galaxy can not directly take as input a paired collection but expect 2 collections: 1 with forward data and 1 with reverse. Before launching MetaPhlAn, we need to split our input paired collection.</p> <p>Hands-on: Assign taxonomic labels with MetaPhlAn</p> <ol> <li>Unzip collection \u2013 Tool link with the following parameters:</li> <li> <p>\"Paired input to unzip\": Input paired collection</p> </li> <li> <p>MetaPhlAn \u2013 Tool link with the following parameters:</p> </li> <li>In \"Inputs\"<ul> <li>\"Input(s)\": <code>Fasta/FastQ file(s) with microbiota reads</code></li> <li>\"Fasta/FastQ file(s) with microbiota reads\": <code>Paired-end files</code><ul> <li>\"Forward paired-end Fasta/FastQ file with microbiota reads\": output of Unzip collection with forward in the name</li> <li>\"Reverse paired-end Fasta/FastQ file with microbiota reads\": output of Unzip collection with reverse in the name</li> </ul> </li> </ul> </li> <li>In \"Outputs\":<ul> <li>\"Output for Krona?\": <code>Yes</code></li> </ul> </li> </ol> <p>Example</p> <p>5 files and a collection are generated by MetaPhlAn :</p> <ul> <li><code>Predicted taxon relative abundances</code>: tabular files with the community profile</li> </ul> <pre><code>#SampleID    Metaphlan_Analysis      \n#clade_name  NCBI_tax_id     relative_abundance  additional_species\nk__Bacteria  2   100.0   \nk__Bacteria|p__Bacteroidetes     2|976   94.38814    \nk__Bacteria|p__Proteobacteria    2|1224  5.61186     \nk__Bacteria|p__Bacteroidetes|c__CFGB45935    2|976|  94.38814    \nk__Bacteria|p__Proteobacteria|c__Alphaproteobacteria     2|1224|28211    5.61186     \nk__Bacteria|p__Bacteroidetes|c__CFGB45935|o__OFGB45935   2|976||     94.38814    \nk__Bacteria|p__Proteobacteria|c__Alphaproteobacteria|o__Rhodobacterales  2|1224|28211|204455     5.61186     \nk__Bacteria|p__Bacteroidetes|c__CFGB45935|o__OFGB45935|f__FGB45935   2|976|||    94.38814    \nk__Bacteria|p__Proteobacteria|c__Alphaproteobacteria|o__Rhodobacterales|f__Rhodobacteraceae  2|1224|28211|&gt; 204455|    31989     5.61186     \nk__Bacteria|p__Bacteroidetes|c__CFGB45935|o__OFGB45935|f__FGB45935|g__GGB56609   2|976||||   94.38814    \nk__Bacteria|p__Proteobacteria|c__Alphaproteobacteria|o__Rhodobacterales|f__Rhodobacteraceae|g__Phycocomes    2| 1224|    28211|204455|31989|2873978  5.61186     \nk__Bacteria|p__Bacteroidetes|c__CFGB45935|o__OFGB45935|f__FGB45935|g__GGB56609|s__GGB56609_SGB78025  2| 976|||||     94.    38814    \nk__Bacteria|p__Proteobacteria|c__Alphaproteobacteria|o__Rhodobacterales|f__Rhodobacteraceae|g__Phycocomes|     s__Phycocomes_zhengii     2|1224|28211|204455|31989|2873978|2056810   5.61186     \nk__Bacteria|p__Bacteroidetes|c__CFGB45935|o__OFGB45935|f__FGB45935|g__GGB56609|s__GGB56609_SGB78025| t__SGB78025     2|    976||||||     94.38814    \nk__Bacteria|p__Proteobacteria|c__Alphaproteobacteria|o__Rhodobacterales|f__Rhodobacteraceae|g__Phycocomes|     s__Phycocomes_zhengii|t__SGB31485     2|1224|28211|204455|31989|2873978|2056810|  5.61186     \n</code></pre> <p>Each line contains 4 columns:    1. the lineage with different taxonomic levels, from high level taxa (kingdom: <code>k__</code>) to more precise taxa    2. the NCBI taxon ids of the lineage taxonomic level    3. the relative abundance found for our sample for the lineage    4. any additional species</p> <p>Question</p> <ol> <li>Which kindgoms have been identified for JC1A?</li> <li>For JP4D?</li> <li>How is the diversity for JP4D compared to Kraken results?</li> </ol> <p>Solution 1. All reads have been unclassified so no kindgom identified 2. Bacteria, no eukaryotes 3. Diversity is really reduced for JP4D using MetaPhlAn, compared to the one identified with Kraken</p> Success <p>??? question</p> <ul> <li><code>Predicted taxon relative abundances for Krona</code>: same information as the previous files but formatted for &gt; visualization using Krona</li> </ul> <pre><code>Column 1 Column 2    Column 3    Column 4    Column 5    Column 6    Column 7    Column 8    Column 9    Column 10\n94.38814         Bacteria    Bacteroidetes   CFGB45935   OFGB45935   FGB45935    GGB56609    GGB56609 SGB78025   \n5.61186      Bacteria    Proteobacteria  Alphaproteobacteria     Rhodobacterales     Rhodobacteraceae    Phycocomes  Phycocomes zhengii  \n94.38814         Bacteria    Bacteroidetes   CFGB45935   OFGB45935   FGB45935    GGB56609    GGB56609 SGB78025   SGB78025\n5.61186      Bacteria    Proteobacteria  Alphaproteobacteria     Rhodobacterales     Rhodobacteraceae    Phycocomes  Phycocomes zhengii  SGB31485\n</code></pre> <p>Each line represent an identified taxons with 9 columns:    - Column 1: The percentage of reads assigned the taxon    - Column 2-9: The taxon description at the different taxonomic levels from Kindgom to more precise taxa</p> <ul> <li>A collection with the same information as in the tabular file but splitted into different files, one per taxonomic level</li> <li> <p><code>BIOM file</code> with community profile in BIOM format, a common and standard format in microbiomics and used as the input for tools like mothur or Qiime.</p> </li> <li> <p><code>Bowtie2 output</code> and <code>SAM file</code> with the results of the sequence mapping on the reference database</p> </li> </ul> <p>Let's now run Krona to visualize the communities</p> <p>Hands-on: Generate Krona visualisation 1. Krona pie chart \u2013 Tool link with the following parameters:    - \"Type of input data\": <code>tabular</code>    -  \"Input file\": Krona output of Metaphlan</p> <ol> <li>Inspect the generated file</li> </ol> <p>Example</p> <p>As pointed before, the community looks a lot less diverse than with Kraken. This is probably due to the reference database, which is potentially not complete enough yet to identify all taxons. Or there are too few reads in the input data to cover enough marker genes. Indeed, no taxon has been identified for JC1A, which contains much less reads than JP4D. However, Kraken is also known to have high number of false positive. A benchmark dataset or mock community where the community DNA content is known would be required to correctly judge which tool provides the better taxonomic classification.</p>"},{"location":"pages/galaxy/02-taxonomy-galaxy/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we look how to get the community profile from microbiome data. We apply Kraken2 or MetaPhlAn to assign taxonomic labels to two microbiome sample datasets. We then visualize the results using Krona, Pavian and Phinch to analyze and compare the datasets. Finally, we discuss important facts when it comes to choosing the right tool for taxonomic assignment.\u00e5</p>"},{"location":"pages/galaxy/all_galaxy_tutorials/","title":"All galaxy tutorials","text":"<p>Global Training that we follow: https://training.galaxyproject.org/training-material/learning-pathways/metagenomics.html</p>"},{"location":"pages/galaxy/all_galaxy_tutorials/#short-introduction-to-galaxy","title":"Short Introduction to Galaxy","text":"<p>We will follow this short introduction to galaxy ( let's try to follow our ) https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-short/tutorial.html</p> <p>A funny way to look at the quality of your data is to switch these ASCII characters to emoji. We can look at this using the tool FASTQE ( FastQ + Emoji) (https://fastqe.com/)</p> <p>Repo github of this tutorial :  https://github.com/galaxyproject/training-material/tree/main/topics/introduction/tutorials/galaxy-intro-short</p>"},{"location":"pages/galaxy/all_galaxy_tutorials/#quality-control-tutorial","title":"Quality Control Tutorial :","text":"<p>Let's try to do this, with the same dataset.  Take the time to look at all the plots that are rendered when doing fastQC. () Trim multidatasets -&gt;  MultiQC https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html</p>"},{"location":"pages/galaxy/all_galaxy_tutorials/#taxonomic-profiling-and-visualization-of-metagenomic-data","title":"Taxonomic Profiling and Visualization of Metagenomic Data","text":"<p>This tutorial uses the same data as we saw in the presentation (Cuatro Ci\u00e9negas). We use Kraken2, which is a fast and comprehensive taxonomic profiling. Assembly started at 4pm BUT only as good as the reference inside the dataset. https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/taxonomic-profiling/tutorial.html</p>"},{"location":"pages/galaxy/all_galaxy_tutorials/#community-alpha-and-beta-diversity","title":"Community alpha and beta diversity","text":"<p>We use the output from Bracken to Calculate alpha and \u03b2 diversity from microbiome taxonomic data. We use Krakentools  https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/diversity/tutorial.html#prepare-galaxy-and-data</p>"},{"location":"pages/galaxy/all_galaxy_tutorials/#phylogenetic-placement","title":"Phylogenetic placement","text":"<p>Explain the difference with the previous analysis, the difference with this other taxonomic profiling. assigns taxonomy to bins, and compare the results with Kraken results.</p>"},{"location":"pages/galaxy/all_galaxy_tutorials/#assembly","title":"Assembly","text":"<pre><code>Why should genomic data be assembled?\n</code></pre> <p>What is the difference between reads and contigs? How can we assemble a metagenome?</p> <p>https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metagenomics-assembly/tutorial.html Adapt this training with the data from Cuatro Ci\u00e9negas.</p>"},{"location":"pages/introduction/introduction/","title":"Introduction","text":"<p>Welcome to the tutorials! Here we will learn how to make a computational research project reproducible using several different tools, described in the figure below:</p> <p></p> <p>The figure above gives an overview of the different parts of computational reproducibility (data, code, workflow and environment), as well as the various tools that are used for each part; Git is, arguably, integral to all of the parts, but we only listed it in the code section for a less cluttered figure.</p> <p>The course has a tutorial for each of the tools, all made so that they can be completed independently of each other. It is therefore perfectly possible to go through them in whatever order you prefer, but we suggest the following order:</p> <ol> <li>Git</li> <li>Conda</li> <li>Snakemake</li> <li>Nextflow</li> <li>R Markdown</li> <li>Jupyter</li> <li>Containers</li> </ol> <p>You will find the tutorials in the Modules section in the navigation menu.</p> <p>Please make sure to carefully follow the pre-course setup to install the tools and download the course material before starting with any of the tutorials. These will create quite a lot of files on your computer, some of which will actually take up a bit of storage space too. In order to remove any traces of these after completing the tutorials, please refer to the Take down section.</p> <p>Before going into the tutorials themselves, we first describe the case study from which the example data comes from.</p>"},{"location":"pages/introduction/introduction/#the-case-study","title":"The case study","text":"<p>We will be running a small bioinformatics project as a case study, and use that to exemplify the different steps of setting up a reproducible research project. To give you some context, the study background and analysis steps are briefly described below.</p>"},{"location":"pages/introduction/introduction/#background","title":"Background","text":"<p>The data is taken from Osmundson, Dewell, and Darst (2013), who have studied methicillin-resistant Staphylococcus aureus (MRSA). MRSA is resistant to\u00a0broad spectrum beta-lactam antibiotics and lead to difficult-to-treat\u00a0infections\u00a0in humans. Lytic bacteriophages have been suggested as potential therapeutic agents, or as the source of novel antibiotic proteins or peptides. One such protein, gp67, was identified as a transcription-inhibiting transcription factor with an antimicrobial effect. To identify S. aureus genes repressed by gp67, the authors expressed gp67 in S. aureus cells. RNA-seq was then performed on three S. aureus strains:</p> <ul> <li>RN4220 with pRMC2 with gp67</li> <li>RN4220 with empty pRMC2</li> <li>NCTC8325-4</li> </ul>"},{"location":"pages/introduction/introduction/#analysis","title":"Analysis","text":"<p>The graph below shows the different steps of the analysis that are included in this project:</p> <p></p> <p>The input files are:</p> <ul> <li>RNA-seq raw data (FASTQ files) for the three strains</li> <li>S. aureus genome sequence (a FASTA file)</li> <li>S. aureus genome annotation (a GFF file)</li> </ul> <p>The workflow itself will perform the following tasks:</p> <ul> <li>Downloading and indexing of the reference genome using Bowtie2</li> <li>Downloading the raw FASTQ data from the Sequence Read Archive (SRA)</li> <li>Run some quality controls on the data using FastQC and MultiQC</li> <li>Align the raw data to the genome and calculate the gene expression using   featureCounts</li> <li>Produce supplementary materials using data from quality controls, gene   expression and the workflow figure shown above</li> </ul>"},{"location":"pages/jupyter/jupyter-1-introduction/","title":"Jupyter 1 introduction","text":"<p>The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain code, equations, visualizations and text. The functionality is partly overlapping with R Markdown (see the tutorial), in that they both use markdown and code chunks to generate reports that integrate results of computations with the code that generated them. Jupyter Notebook comes from the Python community while R Markdown was developed by RStudio, but you could use most common programming languages in either alternative. In practice though, it's quite common that R developers use Jupyter but probably not very common that Python developers use RStudio. Some reasons to use Jupyter include:</p> <ul> <li>Python is lacking a really good IDE for doing exploratory scientific data   analysis, like RStudio or Matlab. Some people use Jupyter simply as an   alternative for that.</li> <li>The community around Jupyter notebooks is large and dynamic, and there are   lots of tools for sharing, displaying or interacting with notebooks.</li> <li>An early ambition with Jupyter notebooks (and its predecessor IPython   notebooks) was to be analogous to the lab notebook used in a wet lab. It   would allow the data scientist to document his or her day-to-day work and   interweave results, ideas, and hypotheses with the code. From   a reproducibility perspective, this is one of the main advantages.</li> <li>Jupyter notebooks can be used, just like R Markdown, to provide a tighter   connection between your data and your results by integrating results of   computations with the code that generated them. They can also do this in an   interactive way that makes them very appealing for sharing with others.</li> </ul> <p>As always, the best way is to try it out yourself and decide what to use it for!</p> <p>This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already. Then open up a terminal and go to <code>workshop-reproducible-research/tutorials/jupyter</code> and activate your <code>jupyter-env</code> Conda environment.</p> <p>A note on nomenclature</p> <ul> <li>Jupyter: a project to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages. Lives at jupyter.org.</li> <li>Jupyter Notebook: A web application that you use for creating and managing notebooks. One of the outputs of the Jupyter project.</li> <li>Jupyter notebook: The actual <code>.ipynb</code> file that constitutes your notebook.</li> </ul>"},{"location":"pages/jupyter/jupyter-2-the-basics/","title":"Jupyter 2 the basics","text":"<p>One thing that sets Jupyter Notebook apart from what you might be used to is that it's a web application, i.e. you edit and run your code from your browser. But first you have to start the Jupyter Notebook server:</p> <pre><code>jupyter notebook --allow-root\n</code></pre> <p>You should see something similar to this printed to your terminal:</p> <pre><code>[I 18:02:26.722 NotebookApp] Serving notebooks from local directory: /Users/john/workshop-reproducible-research/tutorials/jupyter\n[I 18:02:26.723 NotebookApp] 0 active kernels\n[I 18:02:26.723 NotebookApp] The Jupyter Notebook is running at:\n[I 18:02:26.723 NotebookApp] http://localhost:8888/?token=e03f10ccb40efc3c6154358593c410a139b76acf2cae000\n[I 18:02:26.723 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 18:02:26.724 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=e03f10ccb40efc3c6154358593c410a139b76acf2cae785c\n[I 18:02:27.209 NotebookApp] Accepting one-time-token-authenticated connection from ::1\n</code></pre> <p>A note for Windows users</p> <p>If you see the error message <code>Start : This command cannot be run due to the error: The system cannot find the file specified. ...</code> then try starting jupyter with <code>jupyter notebook --no-browser</code> then copy the URL given into the browser directly.</p> <p>Jupyter Notebook probably opened up a web browser for you automatically, otherwise go to the address specified in the message in the terminal. Note that the server is running locally (as <code>http://localhost:8888</code>) so this does not require that you have an active internet connection. Also note that it says:</p> <pre><code>Serving notebooks from local directory: &lt;/some/local/path/workshop-reproducible-research/tutorials/jupyter&gt;\n</code></pre> <p>Everything you do in your Notebook session will be stored in this directory, so you won't lose any work if you shut down the server.</p> <p></p> <p>What you're looking at is the Notebook dashboard. This is where you manage your files, notebooks, and kernels. The Files tab shows the files in your directory. The Running tab keeps track of all your processes. The third tab, Clusters, is used for parallel computing and won't be discussed further in this tutorial. Finally, the Nbextensions tab shows a list of configurable notebook extensions that you can use to add functionality to your notebook (as we'll see below).</p> <p>Let's start by creating an empty notebook by selecting the Files tab and clicking New &gt; Python 3. This will open up a new tab or window looking like this:</p> <p></p> <p>Tip</p> <p>If you want to start Jupyter Notebooks on a cluster that you SSH to (e.g. Uppmax) see the section in the Extra material</p> <p>Jupyter notebooks are made up of cells, and you are currently standing in the first cell in your notebook. The fact that it has a green border indicates that it's in \"Edit mode\", so you can write stuff in it. A blue border indicates \"Command mode\" (see below). Cells in Jupyter notebooks can be of two types: markdown or code.</p> <ul> <li>Markdown: These cells contain static material such as captions, text, lists, images and so on. You express this using Markdown, which is a lightweight markup language. Markdown documents can then be converted to other formats for viewing (the document you're reading now is written in Markdown and then converted to HTML). The format is discussed a little more in detail in the R Markdown tutorial. Jupyter Notebook uses a dialect of Markdown called Github Flavored Markdown, which is described here.</li> <li>Code: These are the cells that actually do something, just as code chunks   do in R Markdown. You can write code in dozens of languages and all do all   kinds of clever tricks. You then run the code cell and any output the code   generates, such as text or figures, will be displayed beneath the cell. We   will get back to this in much more detail, but for now it's enough to   understand that code cells are for executing code that is interpreted by   a kernel (in this case the Python version in your Conda environment).</li> </ul> <p>Before we continue, here are some shortcuts that can be useful. Note that they are only applicable when in command mode (blue frames). Most of them are also available from the menus. These shortcuts are also available from the Help menu in your notebook (there's even an option there to edit shortcuts).</p>  Shortcut   Effect   `enter`   enter Edit mode   `escape`   Enter Command mode   `ctrl` - `enter`  Run the cell   `shift` - `enter`  Run the cell and select the cell below   `alt` - `enter`  Run the cell and insert a new cell below   `s`  Save the notebook   `tab`   For code completion or indentation   `m`, `y`  Toggle between Markdown and Code cells   `d`- `d`   Delete a cell   `a`   Insert cells above current cell   `b`   Insert cells below current cell   `x`   Cut currently selected cells   `o`   Toggle output of current cell"},{"location":"pages/jupyter/jupyter-2-the-basics/#writing-markdown","title":"Writing markdown","text":"<p>Let's use our first cell to create a header. Change the format from Code to Markdown using the drop-down list in the Notebook Toolbar, or by pressing the <code>m</code> key when in command mode. Double click on the cell, or hit <code>enter</code> to enter editing mode (green frame) and input \"# My notebook\" (\"#\" is used in Markdown for header 1). Run the cell with <code>ctrl</code>-<code>enter</code>.</p> <p>Tada!</p> <p>Markdown is a simple way to structure your notebook into sections with descriptive notes, lists, links, images etc.</p> <p>Below are some examples of what you can do in markdown. Paste all or parts of it into one or more cells in your notebook to see how it renders. Make sure you set the cell type to Markdown.</p> <pre><code>## Introduction\nIn this notebook I will try out some of the **fantastic** concepts of Jupyter\nNotebooks.\n\n## Markdown basics\nExamples of text attributes are:\n\n* *italics*\n* **bold**\n* `monospace`\n\nSections can be separated by horizontal lines.\n\n---\n\nBlockquotes can be added, for instance to insert a Monty Python quote:\n\n    Spam!\n    Spam!\n    Spam!\n    Spam!\n\nSee [here](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) for more information.    \n</code></pre>"},{"location":"pages/jupyter/jupyter-2-the-basics/#writing-code","title":"Writing code","text":"<p>Now let's write some code! Since we chose a Python kernel, Python would be the native language to run in a cell. Enter this code in the second cell and run it:</p> <pre><code>print(\"Hello world!\")\n</code></pre> <p>Note how the output is displayed below the cell. This interactive way of working is one of the things that sets Jupyter Notebook apart from RStudio and R Markdown. R Markdown is typically rendered top-to-bottom in one run, while you work in a Jupyter notebook in a different way. This has partly changed with newer versions of RStudio, but it's probably still how most people use the two tools.</p> <p>What is a Jupyter notebook? Let's look a little at the notebook we're currently working in. Jupyter Notebooks are autosaved every minute or so, so you will already have it available. We can be a little meta and do this from within the notebook itself. We do it by running some shell commands in the third code cell instead of Python code. This very handy functionality is possible by prepending the command with <code>!</code>. Try <code>!ls</code> to list the files in the current directory.</p> <p>Aha, we have a new file called <code>Untitled.ipynb</code>! This is our notebook. Look at the first ten lines of the file by using <code>!head Untitled.ipynb</code>. Seems like it's just a plain old JSON file. Since it's a text file it's suitable for version control with for example Git. It turns out that Github and Jupyter notebooks are the best of friends, as we will see more of later. This switching between languages and whatever-works mentality is very prominent within the Jupyter notebook community.</p> <p>Variables defined in cells become variables in the global namespace. You can therefore share information between cells. Try to define a function or variable in one cell and use it in the next. For example:</p> <pre><code>def print_me(str):\n    print(str)\n</code></pre> <p>... and ...</p> <pre><code>print_me(\"Hi!\")\n</code></pre> <p>Your notebook should now look something like this.</p> <p></p> <p>The focus of this tutorial is not on how to write Markdown or Python; you can make really pretty notebooks with Markdown and you can code whatever you want with Python. Rather, we will focus on the Jupyter Notebook features that allow you to do a little more than that.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>That a Jupyter notebook consists of a series of cells, and that they can be either markdown or code cells.</li> <li>That we execute the code in a code cell with the kernel that we chose when opening the notebook.</li> <li>We can run shell commands by prepending them with <code>!</code>.</li> <li>A Jupyter notebook is simply a text file in JSON format.</li> </ul>"},{"location":"pages/jupyter/jupyter-3-magics/","title":"Jupyter 3 magics","text":"<p>Magics constitute a simple command language that significantly extends the power of Jupyter notebooks. There are two types of magics:</p> <ul> <li>Line magics: Commands that are prepended by <code>%</code>, and whose arguments only   extend to the end of the line.</li> <li>Cell magics: Commands that start with <code>%%</code> and then applies to the whole   cell. Must be written on the first line of a cell.</li> </ul> <p>Now list all available magics with <code>%lsmagic</code> (which itself is a magic). You add a question mark to a magic to show the help (e.g. <code>%lsmagic?</code>). Some of them act as shortcuts for commonly used shell commands (<code>%ls</code>, <code>%cp</code>, <code>%cat</code>, ..). Others are useful for debugging and optimizing your code (<code>%timeit</code>, <code>%debug</code>, <code>%prun</code>, ..). For more information see the magics documentation.</p> <p>A very useful magic, in particular when using shell commands a lot in your work, is <code>%%capture</code>. This will capture the stdout/stderr of any code cell and store them in a Python object. Run <code>%%capture?</code> to display the help and try to understand how it works. Try it out with either some Python code, other magics or shell commands. Here is an example of how you can make it work:</p> <pre><code>%%capture output\n%%bash\necho \"Print to stdout\"\necho \"Print to stderr\" &gt;&amp;2\n</code></pre> <p>... and in another cell:</p> <pre><code>print(\"stdout:\" + output.stdout)\nprint(\"stderr:\" + output.stderr)\n</code></pre> <p>Tip</p> <p>You can capture the output of some magics directly like this: <code>my_dir = %pwd</code>.</p> <p>The <code>%%script</code> magic is used for specifying a program (bash, perl, ruby, ..) with which to run the code (similar to a shebang). For some languages it's possible to use these shortcuts:</p> <ul> <li><code>%%ruby</code></li> <li><code>%%perl</code></li> <li><code>%%bash</code></li> <li><code>%%html</code></li> <li><code>%%latex</code></li> <li><code>%%R</code> (here you have to first install the rpy2 extension, for example with   Conda, and then load with <code>%load_ext rpy2.ipython</code>)</li> </ul> <p>Try this out if you know any of the languages above. Otherwise you can always try to print the quadratic formula with LaTeX!</p> <pre><code>\\begin{array}{*{20}c} {x = \\frac{{ - b \\pm \\sqrt {b^2 - 4ac} }}{{2a}}} &amp; {{\\rm{when}}} &amp; {ax^2 + bx + c = 0} \\\\ \\end{array} \n</code></pre> <p>Another useful magic is <code>%precision</code> which sets the floating point precision in the notebook. As a quick example, add the following to a cell and run it:</p> <pre><code>float(100/3)\n</code></pre> <p>Next set the precision to 4 decimal points by running a cell with:</p> <pre><code>%precision 4\n</code></pre> <p>Now run the cell with <code>float(100/3)</code> again to see the difference.</p> <p>Running <code>%precision</code> without additional arguments will restore the default.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>The basics of Jupyter magics and the difference between line magics and cell magics</li> <li>How to capture and use output from notebook cells with <code>%%capture</code></li> <li>How to use magics to run non-Python code in notebooks</li> </ul>"},{"location":"pages/jupyter/jupyter-4-plotting/","title":"Jupyter 4 plotting","text":"<p>An essential feature of Jupyter Notebooks is of course the ability to visualize data and results via plots. A full guide to plotting in Python is beyond the scope of this course, but we'll offer a few glimpses into the plotting landscape of Python.</p> <p>First of all, Python has a library for plotting called matplotlib, which comes packed with functionality for creating high-quality plots. Below is an example of how to generate a line plot of a sine wave.</p> <pre><code># Import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Generate a set of evenly spaced numbers between 0 and 100\nx = np.linspace(0,3*np.pi,100)\n# Use the sine function to generate y-values\ny = np.sin(x)\n# Plot the data\nline, = plt.plot(x, y, color='red', linestyle=\"-\")\n</code></pre> <p>By default plots are rendered in the notebook as rasterized images which can make the quality poor. To render in scalable vector graphics format use the <code>set_matplotlib_formats</code> from the matplotlib_inline package:</p> <pre><code>import matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')\n</code></pre> <p>Now try running the code for the sine wave plot again.</p>"},{"location":"pages/jupyter/jupyter-4-plotting/#other-packages-for-plotting","title":"Other packages for plotting","text":"<p>As we mentioned Matplotlib comes with a lot of functionality which is great because it allows you to create all sorts of plots and modify them exactly to your liking. However, this can also mean that creating very basic plots might involve a lot of cumbersome coding, when all you want is a simple bar chart!</p> <p>Fortunately there are a number of Python packages that build upon matplotlib but with a much simplified interface. One such popular package is seaborn. Below we'll see how to generate a nice looking bar plot with error bars.</p> <p>First import the seaborn package (using an abbreviated name to simplify typing):</p> <pre><code>import seaborn as sns\n</code></pre> <p>Next we'll load some example data of penguins collected at the Palmer Station, in Antarctica.</p> <pre><code>penguins = sns.load_dataset(\"penguins\")\n# Look at first 5 lines of the data\npenguins.head(5)\n</code></pre> <p>The most basic way to generate a bar plot of this data with seaborn is:</p> <pre><code>sns.barplot(data=penguins)\n</code></pre> <p>Simple right? Yes, but maybe not very informative. Here seaborn simply calculates the mean of all numeric variables for the penguins and plots them with error bars representing a 95% confidence interval.</p> <p>Let's say that instead we want to plot the mean value of the body mass of the penguins at the different islands where they were examined.</p> <pre><code>sns.barplot(data=penguins, x=\"island\", y=\"body_mass_g\", ci=\"sd\", errwidth=.5);\n</code></pre> <p>Here we specified to use values in the 'island' column as categories for the x-axis, and values in the 'body_mass_g' column as values for the y-axis. The barplot function of seaborn will then calculate the mean body mass for each island and plot the bars. With <code>ci=\"sd\"</code> we tell the function to draw the standard deviation as error bars, instead of computing a confidence interval. Finally <code>errwidth=.5</code> sets the linewidth of the error bars.</p> <p>If we instead want to visualize the data as a scatterplot we can use the <code>sns.scatterplot</code> function. Let's plot the body mass vs. bill length for all penguins and color the data points by species. We'll also move the legend outside of the plotting area and modify the x and y-axis labels:</p> <pre><code># Store the matplotlib axes containing the plot in a variable called 'ax'\nax = sns.scatterplot(data=penguins, x=\"bill_length_mm\", y=\"body_mass_g\",\n                     hue=\"species\")\n# Modify the labels of the plot\nax.set_xlabel(\"Bill length (mm)\")\nax.set_ylabel(\"Body mass (g)\")\n# Set legend position outside of plot\nax.legend(bbox_to_anchor=(1,1));\n</code></pre> <p>If you want to save a plot to file you can use the <code>plt.savefig</code> function. Add the following to the bottom of the cell with the scatterplot code:</p> <pre><code>plt.savefig(\"scatterplot.pdf\", bbox_inches=\"tight\")\n</code></pre> <p>The <code>bbox_inches=\"tight\"</code> setting ensures that the figure is not clipped when saved to file.</p> <p>The Seaborn website contains great tutorials and examples of other ways to plot data!</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to generate simple plots with <code>matplotlib</code></li> <li>How to import and use the <code>seaborn</code> package for plotting</li> <li>How to save plots from notebooks to a file</li> </ul>"},{"location":"pages/jupyter/jupyter-5-widgets/","title":"Jupyter 5 widgets","text":"<p>Since we're typically running our notebooks in a web browser, they are quite well suited for also including more interactive elements. A typical use case could be that you want to communicate some results to a collaborator or to a wider audience, and that you would like them to be able to modify how the results are displayed. It could, for example, be to select which gene to plot for, or to see how some parameter value affects a clustering. Jupyter notebooks has great support for this in the form of widgets.</p> <p>Widgets are eventful Python objects that have a representation in the browser, often as a control like a slider, textbox, etc. These are implemented in the <code>ipywidgets</code> package.</p> <p>The easiest way to get started with using widgets are via the <code>interact</code> and <code>interactive</code> functions. These functions autogenerate widgets from functions that you define, and then call those functions when you manipulate the widgets. Too abstract? Let's put it into practice!</p> <p>Let's try to add sliders that allow us to change the frequency, amplitude and phase of the sine curve we plotted previously.</p> <pre><code># Import the interactive function from ipywidgets\nfrom ipywidgets import interactive\n# Also import numpy (for calculating the sine curve)\n# and pyplot from matplotlib for plotting\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function for plotting the sine curve\ndef sine_curve(A, f, p):\n    # Set up the plot\n    plt.figure(1, figsize=(4,4))\n    # Create a range of 100 evenly spaced numbers between 0 and 100\n    x = np.linspace(0,10,100)\n    # Calculate the y values using the supplied parameters\n    y = A*np.sin(x*f+p)\n    # Plot the x and y values ('r-' specifies color and line style)\n    plt.plot(x, y, color='red', linestyle=\"-\")\n    plt.show()\n\n# Here we supply the sine_curve function to interactive,\n# and set some limits on the input parameters\ninteractive_plot = interactive(sine_curve,\n            A=(1, 5, 1),\n            f=(0, 5, 1),\n            p=(1, 5, 0.5))\n\n# Display the widgets and the plot\ninteractive_plot\n</code></pre> <p>The code above defines a function called <code>sine_curve</code> which takes three arguments:</p> <ul> <li><code>A</code> = the amplitude of the curve</li> <li><code>f</code> = the frequency of the curve</li> <li><code>p</code> = the phase of the curve</li> </ul> <p>The function creates a plot area, generates x-values and calculates y-values using the <code>np.sin</code> function and the supplied parameters. Finally, the x and y values are plotted.</p> <p>Below the function definition we use <code>interactive</code> with the <code>sine_curve</code> function as the first parameter. This means that the widgets will be tied to the <code>sine_curve</code> function. As you can see we also supply the <code>A</code>, <code>f</code> and <code>p</code> keyword arguments. Importantly, all parameters defined in the <code>sine_curve</code> function must be given in the <code>interactive</code> call and a widget is created for each one.</p> <p>Depending on the <code>type</code> of the passed argument different types of widgets will be created by <code>interactive</code>. For instance:</p> <ul> <li><code>int</code> or <code>float</code> arguments will generate a slider</li> <li><code>bool</code> arguments (True/False) will generate checkbox widgets</li> <li><code>list</code> arguments will generate a dropdown</li> <li><code>str</code> arguments will generate a text-box</li> </ul> <p>By supplying the arguments in the form of tuples we can adjust the properties of the sliders. <code>f=(1, 5, 1)</code> creates a widget with minimum value of <code>1</code>, maximum value of <code>5</code> and a step size of <code>1</code>. Try adjusting these numbers in the <code>interactive</code> call to see how the sliders change (you have to re-execute the cell).</p> <p>The final line of the cell (<code>interactive_plot</code>) is where the actual widgets and plot are displayed. This code can be put in a separate cell, so that you can define functions and widgets in one part of your notebook, and reuse them somewhere else.</p> <p>This is how it should look if everything works. You can now set the frequency amplitude and phase of the sine curve by moving the sliders.</p> <p></p> <p>There are lots of widgets, e.g.:</p> <ul> <li>Dropdown menus</li> <li>Toggle buttons</li> <li>Range sliders</li> <li>File uploader</li> </ul> <p>... and much, much more. Here is a list of all available widgets together with documentation and examples. Some of these widgets cannot be autogenerated by <code>interactive</code>, but fear not! Instead of relying on autogeneration we can define the widget and supply it directly to <code>interactive</code>.</p> <p>To see this in practice, change out the <code>A</code> argument to a pre-defined <code>IntSlider</code> widget. First define the slider:</p> <pre><code>from ipywidgets import widgets\nA = widgets.IntSlider(value=2, min=1, max=5, step=1)\n</code></pre> <p>Then replace the call to <code>interactive</code> so that it looks like this:</p> <pre><code>interactive_plot = interactive(sine_curve, A=A, f=5, p=5)\n</code></pre>"},{"location":"pages/jupyter/jupyter-5-widgets/#extra-challenge","title":"Extra challenge","text":"<p>If you can't get enough of widgets you might want to try this out: see if you can figure out how to add a widget that lets you pick the color for the sine curve line. Search for the appropriate widget in the Widget list. You'll need to update the <code>sine_curve</code> function and pass the new widget as an argument in the call to <code>interactive</code>. If you need help, see the code chunk below:</p> Click to show the solution <pre><code># Import the interactive function from ipywidgets\nfrom ipywidgets import interactive\n# Also import numpy (for calculating the sine curve)\n# and pyplot from matplotlib for plotting\nimport numpy as np\nfrom ipywidgets import widgets ## &lt;- import widgets\nimport matplotlib.pyplot as plt\n\n# Define the function for plotting the sine curve\ndef sine_curve(A, f, p, color): ## &lt;- add parameter here\n    # Set up the plot\n    plt.figure(1, figsize=(4,4))\n    # Create a range of 100 evenly spaced numbers between 0 and 100\n    x = np.linspace(0,10,100)\n    # Calculate the y values using the supplied parameters\n    y = A*np.sin(x*f+p)\n    # Plot the x and y values\n    plt.plot(x, y, color=color) ## &lt;- Use color from widget here\n    plt.show()\n\n# Here we supply the sine_curve function to interactive,\n# and set some limits on the input parameters\n# Define the colorpicker widget\ncolorpicker = widgets.ColorPicker(description='color',value=\"red\")\ninteractive_plot = interactive(sine_curve,\n            A=(1, 5, 1),\n            f=(0, 5, 1),\n            p=(1, 5, 0.5),\n            color=colorpicker) ## &lt;- Supply the colorpicker to the function\n\n# Display the widgets and the plot\ninteractive_plot\n</code></pre> <p>Warning</p> <p>Note that you may have to close the color picker once you've made your choice in order to make the plot update.</p>"},{"location":"pages/jupyter/jupyter-5-widgets/#other-interactive-plots","title":"Other interactive plots","text":"<p>Jupyter widgets, like we used here, is the most vanilla way of getting interactive graphs in Jupyter notebooks. Some other alternatives are:</p> <ul> <li>Plotly is actually an   API to a web service that renders your graph and returns it for display in   your Jupyter notebook. Generates very visually appealing graphs, but from   a reproducibility perspective it's maybe not a good idea to be so reliant on   a third party.</li> <li>Bokeh   is another popular tool for interactive graphs. Most plotting packages for   Python are built on top of matplotlib, but Bokeh has its own library. This   can give a steeper learning curve if you're used to the standard packages.</li> <li>mpld3 tries to integrate matplotlib with   Javascript and the D3js package. It doesn't scale well for very large   datasets, but it's easy to use and works quite seamlessly.</li> </ul> <p>Quick recap</p> <p>In the three previous sections we've learned:</p> <ul> <li>How to implement interactive widgets in notebooks</li> </ul>"},{"location":"pages/jupyter/jupyter-6-extensions/","title":"Jupyter 6 extensions","text":"<p>Jupyter Notebook extensions are add-ons that can increase the functionality of your notebooks. These were installed in the setup section for this tutorial by including the <code>jupyter_contrib_nbextensions</code> package in the conda environment file. You can read more about the extensions here.</p> <p>To manage extensions go to the Jupyter dashboard in your browser and click the Nbextensions tab. You should see something similar to this:</p> <p></p> <p>Clicking an extension in the list displays more information about it. To enable/disable extensions simply click the checkbox next to the extension name in the list. Some useful extensions include</p> <ul> <li> <p>Hide input all, which allows you to hide all code cells with the click of   a button.</p> </li> <li> <p>Collapsible Headings, which allows you to collapse sections below markdown   headings to increase readability.</p> </li> <li> <p>Table of Contents (2), which adds a table of contents to the notebook   making navigation a lot quicker especially for long notebooks.</p> </li> </ul> <p>Feel free to peruse the list and find your own favourites! Keep in mind that these are unofficial, community-contributed extensions and as such they come with few, if any, guarantees.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>What Jupyter extensions are and how to enable/disable them</li> </ul>"},{"location":"pages/jupyter/jupyter-7-converting-notebooks/","title":"Jupyter 7 converting notebooks","text":"<p>Notebooks can be converted to various output formats such as HTML, PDF, LaTeX etc. directly from the File -&gt; Download as menu.</p> <p>Conversion can also be performed on the command line using the <code>jupyter nbconvert</code> command. <code>nbconvert</code> is installed together with the <code>jupyter</code> Conda package and is executed on the command line by running <code>jupyter nbconvert</code>.</p> <p>The syntax for converting a Jupyter notebook is:</p> <pre><code>jupyter nbconvert --to &lt;FORMAT&gt; notebook.ipynb\n</code></pre> <p>Here <code>&lt;FORMAT&gt;</code> can be any of <code>asciidoc</code>, <code>custom</code>, <code>html</code>, <code>latex</code>, <code>markdown</code>, <code>notebook</code>, <code>pdf</code>, <code>python</code>, <code>rst</code>, <code>script</code>, <code>slides</code>. Converting to some output formats (e.g. PDF) may require you to install separate software such as Pandoc or a TeX environment.</p> <p>Try converting the <code>Untitled.ipynb</code> notebook that you have been working on so far to HTML using <code>jupyter nbconvert</code>.</p> <p>Tip</p> <p>To export notebooks in the form they appear with Jupyter Extensions activated you can make use of the <code>nbextensions</code> template that is installed with the <code>jupyter_contrib_nbextensions</code> package. Adding <code>--template=nbextensions</code> to the <code>jupyter nbconvert</code> call should do the trick, but note that not all extensions are guaranteed to display right after exporting.</p> <p><code>nbconvert</code> can also be used to run a Jupyter notebook from the command line by running:</p> <pre><code>jupyter nbconvert --execute --to &lt;FORMAT&gt; notebook.ipynb\n</code></pre> <p><code>nbconvert</code> executes the cells in a notebook, captures the output and saves the results in a new file. Try running it on the <code>Untitled.ipynb</code> notebook.</p> <p>You can also specify a different output file with <code>--output &lt;filename&gt;</code>.</p> <p>So in order to execute your <code>Untitled.ipynb</code> notebook and save it to a file named <code>report.html</code> you could run:</p> <pre><code>jupyter nbconvert --to html --output report.html --execute Untitled.ipynb\n</code></pre> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to convert Jupyter notebooks to various other formats</li> <li>How to use <code>nbconvert</code> to convert notebooks on the command line</li> </ul>"},{"location":"pages/jupyter/jupyter-8-the-mrsa-case-study/","title":"Jupyter 8 the mrsa case study","text":"<p>As you might remember from the intro, we are attempting to understand how lytic bacteriophages can be used as a future therapy for the multiresistant bacteria MRSA (methicillin-resistant Staphylococcus aureus). We have already seen how to define the project environment in the Conda tutorial and how to set up the workflow in the Snakemake tutorial. Here we explore the results from the Snakemake tutorial and generate a Supplementary Material file with some basic stats.</p> <p>In the <code>workshop-reproducible-research/tutorials/jupyter/</code> directory you will find a notebook called <code>supplementary_material.ipynb</code>. Open this notebook with Jupyter by running:</p> <pre><code>jupyter notebook supplementary_material.ipynb\n</code></pre> <p>Tip</p> <p>Using what you've learned about markdown in notebooks, add headers and descriptive text to subdivide sections as you add them. This will help you train how to structure and keep note of your work with a notebook.</p> <p>You will see that the notebook contains only a little markdown text and a code cell with a function <code>get_geodata</code>. We'll start by adding a cell with some import statements.</p> <p>Run the cell with the <code>get_geodata</code> function and add a new cell directly after it. Then add the following to the new cell:</p> <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n</code></pre> <p>This imports the <code>pandas</code> (for working with tables), <code>seaborn</code> and <code>matplotlib.pyplot</code> (for plotting) and <code>numpy</code> (for numerical operations) Python modules.</p> <p>Also add:</p> <pre><code>import matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')\n</code></pre> <p>to set high-quality output for plots.</p> <p>Run the cell and create a new one below it.</p> <p>In the next cell we'll define some parameters to use for the notebook:</p> <pre><code>counts_file=\"results/tables/counts.tsv\"\nsummary_file=\"results/tables/counts.tsv.summary\"\nmultiqc_file=\"intermediate/multiqc_general_stats.txt\"\nrulegraph_file=\"results/rulegraph.png\"\nSRR_IDs=[\"SRR935090\",\"SRR935091\",\"SRR935092\"]\nGSM_IDs=[\"GSM1186459\",\"GSM1186460\",\"GSM1186461\"]\nGEO_ID=\"GSE48896\"\n</code></pre> <p>As you can see we add paths to results files and define lists with some sample IDS. Run this cell and add a new one below it.</p> <p>Next, we'll fetch some sample information from NCBI using the <code>get_geodata</code> function defined at the start of the notebook and collate it into a dataframe.</p> <pre><code>id_df = pd.DataFrame(data=GSM_IDs, index=SRR_IDs, columns=[\"geo_accession\"])\ngeo_df = get_geodata(GEO_ID)\nname_df = pd.merge(id_df, geo_df, left_on=\"geo_accession\", right_index=True)\n# Create a dictionary to rename sample ids in downstream plots\nname_dict = name_df.to_dict()\n</code></pre> <p>Run the cell and take a look at the contents of the <code>name_df</code> dataframe (e.g. run a cell with that variable only to output it below the cell).</p> <p>Now we'll load some statistics from the QC part of the workflow, specifically the 'general_stats' file from <code>multiqc</code>. Add the following to a new cell and run it:</p> <pre><code>qc = pd.read_csv(multiqc_file, sep=\"\\t\")\nqc.rename(columns=lambda x: x.replace(\"FastQC_mqc-generalstats-fastqc-\", \"\").replace(\"_\", \" \"), inplace=True)\nqc = pd.merge(qc, name_df, left_on=\"Sample\", right_index=True)\nqc\n</code></pre> <p>In the code above we load the multiqc file, rename the columns by stripping the <code>FastQC_mqc-generalstats-fastqc-</code> part from column names and replace underscores with spaces. Finally the table is merged with the information obtained in the step above and output to show summary statistics from the QC stage.</p> <p>Next it's time to start loading gene count results from the workflow. Start by reading the counts and summary results, then edit the columns and index:</p> <pre><code># Read count data\ncounts = pd.read_csv(counts_file, sep=\"\\t\", header=0, comment=\"#\", index_col=0)\n# Read summary data\ncounts_summary = pd.read_csv(summary_file, sep=\"\\t\", index_col=0)\n# Rename columns to extract SRR ids\ncounts.rename(columns = lambda x: x.split(\"/\")[-1].replace(\".sorted.bam\",\"\"), inplace=True)\ncounts_summary.rename(columns = lambda x: x.split(\"/\")[-1].replace(\".sorted.bam\",\"\"), inplace=True)\n</code></pre> <p>Take a look at the <code>counts</code> dataframe to get an idea of the data structure. As you can see the dataframe shows genes as rows while the columns shows various information such as start and stop, strand and length of the genes. The last three columns contain counts of the genes in each of the samples.</p> <p>If you have a look at the <code>counts_summary</code> dataframe you will see statistics from the read assignment step, showing number of reads that could be properly assigned as well as number of reads that could not be assigned to genes for various reasons.</p> <p>Now let's generate a barplot of the summary statistics. Before we plot, we'll remove rows that have only zero values:</p> <pre><code># Remove rows with only zero values\nsummary_plot_data = counts_summary.loc[counts_summary.sum(axis=1)&gt;0]\n</code></pre> <p>Now for the plotting:</p> <pre><code># Set color palette to 'Set2'\ncolors = sns.color_palette(\"Set2\")\n# Create a stacked barplot\nax = summary_plot_data.T.plot(kind=\"bar\", stacked=True, color=colors)\n# Move legend and set legend title\nax.legend(bbox_to_anchor=(1,1), title=\"Category\");\n</code></pre> <p>The final plot will be a heatmap of gene counts for a subset of the genes. We'll select genes whose standard deviation/mean count across samples is greater than 1.5, and have a maximum of at least 5 reads in 1 or more sample:</p> <pre><code># Slice the dataframe to only sample counts\ncount_data = counts.loc[:, SRR_IDs]\n# Filter to genes with std/mean &gt; 1.2 and with a max of at least 5\nheatmap_data = count_data.loc[(count_data.std(axis=1).div(count_data.mean(axis=1))&gt;1.2)&amp;(count_data.max(axis=1)&gt;5)]\n</code></pre> <p>We'll also replace the SRR ids with the title of samples used in the study, using the <code>name_dict</code> dictionary created further up in the notebook:</p> <pre><code>heatmap_data = heatmap_data.rename(columns = name_dict['title'])\n</code></pre> <p>Now let's plot the heatmap. We'll log-transform the counts, set color scale to Blue-Yellow-Red and cluster both samples and genes using 'complete' linkage clustering:</p> <pre><code>with sns.plotting_context(\"notebook\", font_scale=0.7):\n    ax = sns.clustermap(data=np.log10(heatmap_data+1), cmap=\"RdYlBu_r\",\n                        method=\"complete\", yticklabels=True, linewidth=.5,\n                        cbar_pos=(.7, .85, .05, .1), figsize=(3,9))\n    plt.setp(ax.ax_heatmap.get_xticklabels(), rotation=270)\n</code></pre> <p>In the code above we use the seaborn <code>plotting_context</code> function to scale all text elements of the heatmap in one go.</p> <p>As a final step we'll add some info for reproducibility under the Reproducibility section. To add the overview image of the workflow found in <code>results/rulegraph.png</code> we can use the <code>Image</code> function from <code>IPython.display</code>:</p> <pre><code>from IPython.display import Image\nImage(rulegraph_file)\n</code></pre> <p>Let's also output the full conda environment so that all packages and versions are included in the notebook. There are several ways this can be done, for example you could simply add:</p> <pre><code>!conda list\n</code></pre> <p>to the end of the notebook.</p> <p>Tip</p> <p>If you want to know more about how notebooks can be integrated into Snakemake worfklows, see the Extra material at the end of this tutorial</p>"},{"location":"pages/jupyter/jupyter-8-the-mrsa-case-study/#sharing-your-work","title":"Sharing your work","text":"<p>The files you're working with come from a GitHub repo. Both GitHub and Bitbucket can render Jupyter notebooks as well as other types of Markdown documents. Now go to our GitHub repo at https://github.com/NBISweden/workshop-reproducible-research and navigate to <code>tutorials/jupyter/supplementary_material.ipynb</code>.</p> <p></p> <p>As you can imagine, having this very effortless way of sharing results can greatly increase the visibility of your work. You work as normal on your project, and push regularly to the repository as you would anyways, and the output is automatically available for anyone to see. Or for a select few if you're not ready to share your findings with the world quite yet.</p> <p>Say your notebook isn't on Github/Bitbucket. All hope isn't lost there. Jupyter.org provides a neat functionality called nbviewer, where you can paste a URL to any notebook and they will render it for you. Go to https://nbviewer.jupyter.org and try this out with our notebook.</p> <pre><code>https://raw.githubusercontent.com/NBISweden/workshop-reproducible-research/main/tutorials/jupyter/supplementary_material.ipynb\n</code></pre>"},{"location":"pages/jupyter/jupyter-8-the-mrsa-case-study/#shared-interactive-notebooks","title":"Shared interactive notebooks","text":"<p>So far we've only shared static representations of notebooks. A strong trend at the moment is to run your notebooks in the cloud, so that the person you want to share with could actually execute and modify your code. This is a great way of increasing visibility and letting collaborators or readers get more hands-on with your data and analyses. From a reproducibility perspective, there are both advantages and drawbacks. On the plus side is that running your work remotely forces you to be strict when it comes to defining the environment it uses (probably in the form of a Conda environment or Docker image). On the negative side is that you become reliant on a third-party service that might change input formats, go out of business, or change payment model.</p> <p>Here we will try out a service called Binder, which lets you run and share Jupyter Notebooks in Git repositories for free. There are a number of example repositories that are setup to be used with Binder. Navigate to https://github.com/binder-examples/conda/ to see one such example. As you can see the repository contains a LICENSE file, a README, an environment file and a notebook. To use a repository with Binder the environment file should contain all the packages needed to run notebooks in the repo. So let's try to run the <code>index.ipynb</code> file using Binder:</p> <p>Just go to https://mybinder.org and paste the link to the GitHub repo. Note the link that you can use to share your notebook. Then press \"launch\".</p> <p></p> <p>What will happen now is that:</p> <ul> <li>Binder detects the <code>environment.yml</code> file in the root of the repo.   Binder then builds a Docker image based on the file. This might take   a minute or two. You can follow the progress in the build log.</li> <li>Binder then launches the Jupyter Notebook server in the Docker   container..</li> <li>..and opens a browser tab with it for you.</li> </ul> <p>Once the process is finished you will be presented with a Jupyter server overview of the contents in the repository. Click on the <code>index.ipynb</code> notebook to open it. Tada! You are now able to interact with (and modify) someone else's notebook online.</p> <p>Applied to your own projects you now have a way to run analyses in the cloud and in an environment that you define yourself. All that's needed for someone to replicate your analyses is that you share a link with them. Note that notebooks on Binder are read-only; its purpose is for trying out and showing existing notebooks rather than making new ones.</p> <p>Tip</p> <p>By default Binder looks for configuration files such as environment.yml in the root of the repository being built. But you may also put such files outside the root by making a <code>binder/</code> folder in the root and placing the file there.  </p> <p>A note on transparency</p> <p>Resources like Github/Bitbucket and Jupyter Notebooks have changed the way we do scientific research by encouraging visibility, social interaction and transparency. It was not long ago that the analysis scripts and workflows in a lab were well-guarded secrets that we only most reluctantly shared with others. Assuming that it was even possible. In most cases, the only postdoc who knew how to get it to work had left for a new position in industry, or no one could remember the password to the file server. If you're a PhD student, we encourage you to embrace this new development wholeheartedly, for it will make your research better and make you into a better scientist. And you will have more fun.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How notebooks can be used to generate summary statistics and plots using the results of a workflow run</li> <li>How to share notebooks via nbviewer and Binder</li> </ul>"},{"location":"pages/jupyter/jupyter-9-extra-material/","title":"Jupyter 9 extra material","text":"<p>Here are some useful resources if you want to read more about Jupyter in general:</p> <ul> <li>The Jupyter project site contains a lot of information   and inspiration.</li> <li>The Jupyter Notebook documentation.</li> <li>A guide to using   widgets for creating interactive notebooks.</li> </ul>"},{"location":"pages/jupyter/jupyter-9-extra-material/#running-jupyter-notebooks-on-a-cluster","title":"Running jupyter notebooks on a cluster","text":"<ul> <li>Login to Uppmax, making sure to use a specific login node, e.g. <code>rackham1</code>:</li> </ul> <pre><code>ssh &lt;your-user-name&gt;@rackham1.uppmax.uu.se\n</code></pre> <ul> <li>Create/activate a conda environment containing <code>jupyter</code> then run:</li> </ul> <pre><code>jupyter notebook\n</code></pre> <p>When the Jupyter server starts up you should see something resembling: <pre><code>[I 11:00:00.000 NotebookApp] Serving notebooks from local directory: &lt;path-to-your-local-dir&gt;\n[I 11:00:00.000 NotebookApp] Jupyter Notebook 6.4.6 is running at:\n[I 11:00:00.000 NotebookApp] http://localhost:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000\n[I 11:00:00.000 NotebookApp]  or http://127.0.0.1:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000\n[I 11:00:00.000 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre></p> <p>Now a Jupyter notebook server is running on the Uppmax end. The line that says: <pre><code>[I 11:00:00.000 NotebookApp] http://localhost:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000\n</code></pre></p> <p>contains information on the port used on the server side (8889 in this case) and the token required to use the server (<code>357d65100058efa40a0641fce7005addcff339876c5e8000</code>).</p> <p>Next step is to use this information to login to the server from your local computer.  </p> <p>On your local computer</p> <p>In a terminal, run the following command to start port forwarding of port 8080 on your local computer to the remote port on the Uppmax side. Replace  with the port given when you started the server on Uppmax. Also replace  with your user name on Uppmax. <pre><code>ssh -N -L localhost:8080:localhost:&lt;remote-port&gt; &lt;your-user-name&gt;@rackham1.uppmax.uu.se\n</code></pre> <p>As long as this process is running the port forwarding is running. To disable it simply interrupt it with <code>CTRL + C</code>.</p> <p>Connect to the jupyter server by opening <code>localhost:8080</code> in your browser. When prompted, paste the token you got when starting the server on Uppmax.</p> <p>You are now (hopefully) accessing the jupyter server that's running on Uppmax, via your local browser.</p>"},{"location":"pages/jupyter/jupyter-9-extra-material/#integrating-notebooks-with-snakemake-workflows","title":"Integrating notebooks with Snakemake workflows","text":"<p>In the case study section of this tutorial we created a Jupyter notebook that used output from a Snakemake workflow and produced some summary results and plots. Wouldn't it be nice if this was actually part of the workflow itself? To generate a HTML version of the notebook we can use what we learned in the section about converting notebooks. The command to execute the notebook and save it in HTML format in a file <code>results/supplementary.html</code> would be:</p> <pre><code>jupyter nbconvert --to HTML --output-dir results --output supplementary.html --execute supplementary_material.ipynb\n</code></pre> <p>This command could be used in a rule, e.g. <code>make_supplementary</code>, the input of which would be <code>results/tables/counts.tsv</code>, <code>intermediate/multiqc_general_stats.txt</code>, and <code>results/rulegraph.png</code>. See if you can work out how to implement such a rule at the end of the <code>Snakefile</code> found in the <code>jupyter/</code> directory. You can find an example in the code chunk below:</p> <pre><code>rule make_supplementary:\n    input:\n        counts = \"results/tables/counts.tsv\",\n        summary = \"results/tables/counts.tsv.summary\",\n        multiqc_file = \"intermediate/multiqc_general_stats.txt\",\n        rulegraph = \"results/rulegraph.png\"\n    output:\n        \"results/supplementary.html\"\n    params:\n        base = lambda wildcards, output: os.path.basename(output[0]),\n        dir = lambda wildcards, output: os.path.dirname(output[0])\n    shell:\n        \"\"\"\n        jupyter nbconvert --to HTML --output-dir {params.dir} --output {params.base} \\\n            --execute supplementary_material.ipynb\n        \"\"\"\n</code></pre> <p>Note</p> <p>The Conda enivronment for the jupyter tutorial does not contain packages required to run the full snakemake workflow. So if you wish to test jupyter integration fully you should update the conda environment by running <code>conda install snakemake-minimal fastqc sra-tools multiqc bowtie2 tbb samtools htseq bedtools wget graphviz</code></p>"},{"location":"pages/jupyter/jupyter-9-extra-material/#more-integrations","title":"More integrations","text":"<p>Snakemake actually supports the execution of notebooks via the <code>notebook:</code> rules directive. See more about Jupyter integration in the snakemake docs. In the <code>notebook:</code> directive of such a rule you specify the path to a jupyter notebook (relative to the Snakefile) which is then executed when the rule is run.</p> <p>So how is this useful?</p> <p>In the notebook itself this gives you access to a <code>snakemake</code> object containing information about input and output files for the rule via <code>snakemake.input</code> and <code>snakemake.output</code>. Similarly, you can access rule wildcards with <code>snakemake.wildcards</code>, params with <code>snakemake.params</code>, and config settings with <code>snakemake.config</code>.</p> <p>When snakemake runs the rule with the <code>notebook:</code> directive <code>jupyter-nbconvert</code> is used to execute the notebook. No HTML output is generated here but it is possible to store a version of the notebook in its final processed form by adding the following to the rule:</p> <pre><code>log:\n    notebook=\"&lt;path&gt;/&lt;to&gt;/&lt;processed&gt;/&lt;notebook.ipynb&gt;\"\n</code></pre> <p>Because you won't get the notebook in full HTML glory, this type of integration is better suited if you want to use a notebook to generate figures and store these in local files (e.g. pdf/svg/png formats).</p> <p>We'll use the <code>supplementary_material.ipynb</code> notebook as an example! Let's say that instead of exporting the entire notebook to HTML we want a rule that outputs pdf versions of the barplot and heatmap figures we created.</p> <p>Let's start by setting up the rule. For simplicity we'll use the same input as when we edited the notebook in the first place. The output will be <code>results/barplot.pdf</code> and <code>results/heatmap.pdf</code>. Let's also output a finalized version of the notebook using the <code>log: notebook=</code> directive:</p> <pre><code>rule make_supplementary_plots:\n    input:\n        counts = \"results/tables/counts.tsv\",\n        summary = \"results/tables/counts.tsv.summary\",\n        multiqc = \"intermediate/multiqc_general_stats.txt\",\n        rulegraph = \"results/rulegraph.png\"\n    output:\n        barplot = \"results/barplot.pdf\",\n        heatmap = \"results/heatmap.pdf\"\n    log:\n        notebook = \"results/supplementary.ipynb\"\n</code></pre> <p>The notebook will now have access to <code>snakemake.input.counts</code>, <code>snakemake.output.barplot</code> and <code>snakemake.output.heatmap</code> when executed from within the workflow. Let's go ahead and edit the notebook! In the cell where we defined notebook parameters edit the code so that it looks like this:</p> <pre><code>counts_file=snakemake.input.counts\nsummary_file=snakemake.input.summary\nmultiqc_file=snakemake.input.multiqc\nrulegraph_file=snakemake.input.rulegraph\n\nSRR_IDs=snakemake.params.SRR_IDs\nGSM_IDs=snakemake.params.GSM_IDs\nGEO_ID=snakemake.params.GEO_ID\n</code></pre> <p>Notice that we set the <code>SRR_IDs</code>, <code>GSM_IDs</code> and <code>GEO_ID</code> variables using variables in <code>snakemake.params</code>? However, we haven't defined these in our rule yet so let's go ahead and do that now. Add the <code>params</code> section so that the <code>make_supplementary_plots</code> in the Snakefile looks like this:</p> <pre><code>rule make_supplementary_plots:\n    input:\n        counts = \"results/tables/counts.tsv\",\n        multiqc = \"intermediate/multiqc_general_stats.txt\",\n        rulegraph = \"results/rulegraph.png\"\n    output:\n        barplot = \"results/barplot.pdf\",\n        heatmap = \"results/heatmap.pdf\"\n    log:\n        notebook = \"results/supplementary.ipynb\"\n    params:\n        SRR_IDs = [\"SRR935090\",\"SRR935091\",\"SRR935092\"],\n        GSM_IDs = [\"GSM1186459\", \"GSM1186460\", \"GSM1186461\"],\n        GEO_ID = \"GSE48896\"\n    notebook: \"supplementary_material.ipynb\"\n</code></pre> <p>Tip</p> <p>One way to further generalize this rule could be to define the SRR_IDs, GSM_IDs and GEO_ID parameters in a config file instead, in which case they would be directly accessible from within the notebook using <code>snakemake.config['SRR_IDs']</code> etc.</p> <p>Now the rule contains everything needed, but we still need to edit the notebook to save the plots to the output files. First, edit the cell that generates the barplot so that it looks like this:</p> <pre><code># Create a stacked barplot\nax = summary_plot_data.T.plot(kind=\"bar\", stacked=True, color=colors)\n# Move legend and set legend title\nax.legend(bbox_to_anchor=(1,1), title=\"Category\");\nplt.savefig(snakemake.output.barplot, dpi=300, bbox_inches=\"tight\") ## &lt;-- Add this line!\n</code></pre> <p>Finally, edit the cell that generates the heatmap so that it looks like this:</p> <pre><code>count_data = counts.loc[:, SRR_IDs]\nheatmap_data = count_data.loc[(count_data.std(axis=1).div(count_data.mean(axis=1))&gt;1.2)&amp;(count_data.max(axis=1)&gt;5)]\nheatmap_data = heatmap_data.rename(columns = name_dict['title'])\nwith sns.plotting_context(\"notebook\", font_scale=0.7):\n    ax = sns.clustermap(data=np.log10(heatmap_data+1), cmap=\"RdYlBu_r\",\n                        method=\"complete\", yticklabels=True, linewidth=.5,\n                        cbar_pos=(.7, .85, .05, .1), figsize=(3,9))\n    plt.setp(ax.ax_heatmap.get_xticklabels(), rotation=270)\n    plt.savefig(snakemake.output.heatmap, dpi=300, bbox_inches=\"tight\") ## &lt;-- Add this line!\n</code></pre> <p>Now you can run the following to generate the plots:</p> <pre><code>snakemake -j 1 make_supplementary_plots\n</code></pre>"},{"location":"pages/jupyter/jupyter-9-extra-material/#presentations-with-jupyter","title":"Presentations with Jupyter","text":"<p>As if all the above wasn't enough you can also create presentations/slideshows with Jupyter! Simply use conda to install the RISE extension to your jupyter environment:</p> <pre><code>conda install -c conda-forge rise\n</code></pre> <p>then open up a notebook of your choice. In the menu click View -&gt; Cell Toolbar -&gt; Slideshow. Now every cell will have a drop-down in the upper right corner allowing you to set the cell type:</p> <ul> <li>Slide: a regular slide</li> <li>Sub-Slide: a regular slide that will be displayed below the previous</li> <li>Fragment: these cells split up slides so that content (fragments) are   added only when you press Space</li> <li>Skip: these cells will not appear in the presentation</li> <li>Notes: these cells act as notes, shown in the speaker view but not in the   main view</li> </ul> <p>The presentation can be run directly from the notebook by clicking the 'Enter/Exit RISE Slideshow' button (looks like a bar chart) in the toolbar, or by using the keyboard shortcut <code>Alt-r</code>. Running it directly from a notebook means you can also edit and run cells during your presentation. The downside is that the presentation is not as portable because it may rely on certain software packages that other people are not comfortable with installing.</p> <p>You can also export the notebook to an HTML-file with <code>jupyter nbconvert --execute --to SLIDES &lt;your-notebook.ipynb&gt;</code>. The resulting file, with the slideshow functionality included, can be opened in any browser. However, in this format you cannot run/edit cells.</p>"},{"location":"pages/metagenomic/00-meta_introduction/","title":"Introduction","text":"<p>This course will be based on the course from Carpentries : https://carpentries-lab.github.io/metagenomics-workshop/</p> <p>The Github Link:  https://github.com/carpentries-lab/metagenomics-workshop?tab=readme-ov-file</p> <p>Github Link to the Markdown files: https://github.com/carpentries-lab/metagenomics-analysis</p>"},{"location":"pages/metagenomic/01-background-metadata/","title":"Starting a Metagenomics Project","text":"<p>Lesson overview</p> <p>Teaching: 15 min Exercises: 15 min  </p> <p>Questions - How do you plan a metagenomics experiment? - How does a metagenomics project look like?</p> <p>Objectives - Learn the differences between shotgun and metabarcoding (amplicon metagenomics) techniques. - Understand the importance of metadata. - Familiarize yourself with the Cuatro Ci\u00e9negas experiment.</p>"},{"location":"pages/metagenomic/01-background-metadata/#metagenomics","title":"Metagenomics","text":"<p>Metagenomes are collections of genomic  sequences from various (micro)organisms that coexist in any  given space. They are like snapshots that can give us information  about the taxonomic and even metabolic or functional composition  of the communities we decide to study. Thus, metagenomes  are usually employed to investigate the ecology of defining  characteristics of niches ( e.g.,, the human gut or the ocean floor). </p> <p>Since metagenomes are mixtures of sequences that belong to different species,  a metagenomic workflow is designed to answer two questions:  1. What species are represented in the sample? 2. What are they capable of doing?</p> <p>To find which species are present in a niche, we must do a taxonomic assignation of the obtained sequences.  To find out their capabilities, we can  look at the genes directly encoded in the metagenome or find the  genes associated with the species that we found. In order to  know which methodology we should use, it is essential to  know what questions we want to answer. </p>"},{"location":"pages/metagenomic/01-background-metadata/#shotgun-and-amplicon","title":"Shotgun and amplicon","text":"<p>There are two paths to obtain information from a complex sample:  1. Shotgun Metagenomics 2. Metabarcoding. </p> <p>Each is named after the sequencing methodology employed  Moreover, have particular use cases with inherent advantages and disadvantages.</p> <p>With Shotgun Metagenomics, we sequence random parts (ideally all of them) of the  genomes present in a sample. We can search the origin of these  pieces (i.e., their taxonomy) and also try to find to what  part of the genome they correspond. Given enough pieces, it is possible  to obtain complete individual genomes from a shotgun metagenome (MAGs),  which could give us a bunch of information about the species  in our study. MAGs assembly, however, requires a lot of genomic  sequences from one organism. Since the sequencing is done at random,  it needs a high depth of community sequencing  to ensure that we obtain enough pieces of a given genome. Required depth gets  exponentially challenging when our species of interest is not very abundant.  It also requires that we have enough DNA to work with, which can be  challenging to obtain in some instances. Finally, sequencing is expensive, and because of this, making technical  and biological replicates can be prohibitively costly.   </p> <p>On the contrary, Metabarcoding tends to be cheaper,  which makes it easier to duplicate and even triplicate  them without taking a big financial hit. The lower cost is because  Metabarcoding is the collection of small genomic fragments  present in the community and amplified through PCR. Ideally, if the  amplified region is present only once in every genome,  we would not need to sequence the amplicon metagenome so thoroughly  because one sequence is all we need to get the information about that genome, and by extension, about that species. On the other  hand, if a genome in the community lacks the region targeted by the  PCR primers, then no amount of sequencing can give us information  about that genome. Conservation across species is why the most popular amplicon used for  this methodology are 16S amplicons for Bacteria since every known  bacterium has this particular region. Other regions can be chosen,  but they are used for specific cases. However, even 16S amplicons  are limited to, well, the 16S region, so amplicon metagenomes cannot  directly tell us a lot about the metabolic functions found in each genome,  although educated guesses can be made by knowing which genes are  commonly found in every identified species. </p> <p> </p>"},{"location":"pages/metagenomic/01-background-metadata/#on-metadata","title":"On Metadata","text":"<p>Once we have chosen an adequate methodology for our study,  we must take extensive notes on the origin of our samples and how we treated them. These notes constitute the metadata, or data about our data,  and they are crucial to understanding and interpreting the results we will obtain later in our metagenomic analysis. Most of the time,  the differences that we observe when comparing metagenomes can be  correlated to the metadata, which is why we must devote a whole section  of our experimental design to the metadata we expect to collect and record carefully. </p> <p>Discussion #1: Choosing amplicon or shotgun sequencing?</p> <p>Suppose you want to find the source of a nasty gut infection in people. Which type of sequencing methodology would you choose? Which type of metadata would be helpful to record?</p> Click to show the answer <p>For a first exploration, 16S is a better idea since you could detect known pathogens by knowing the taxons in the community. Nevertheless, if the disease is the consequence of a viral infection, the pathogen can only be discovered with shotgun metagenomics (that was the case of SARS-CoV 2). Also, metabarcoding does not provide insights into the genetic basis of the pathogenic phenotypes. Metadata will depend on the type of experiment. For this case, some helpful metadata could be sampling methodology, date, place (country, state, region, city, etc.), patient's sex and age, the anatomical origin of the sample, symptoms, medical history, diet, lifestyle, and environment.</p>"},{"location":"pages/metagenomic/01-background-metadata/#cuatro-cienegas","title":"Cuatro Ci\u00e9negas","text":"<p>During this lesson, we will work with actual metagenomic information,  so we should be familiarized with it. The metagenomes that we will  use were collected in Cuatro Ci\u00e9negas, a region that has been extensively studied by Valeria Souza.  Cuatro Ci\u00e9negas is an oasis in the Mexican desert whose  environmental conditions are often linked to the ones present in ancient seas, due to  a higher-than-average content of sulfur and magnesium but a lower  concentrations of phosphorus and other nutrients. Because of these particular  conditions, the Cuatro Ci\u00e9negas basin is a fascinating place to conduct  a metagenomic study to learn more about the bacterial diversity that is capable to survive and thrive in that environment.</p> <p>The particular metagenomic study that we are going to work with was collected in a study about the response of the Cuatro Cienegas' bacterial community to nutrient enrichment.  In this study, authors compared the differences between the microbial community in its natural,  oligotrophic, phosphorus-deficient environment, a pond from the Cuatro Ci\u00e9negas Basin (CCB),  and the same microbial community under a fertilization treatment. The comparison between bacterial  communities showed that many genomic traits, such as mean bacterial genome size, GC content,  total number of tRNA genes, total number of rRNA genes, and codon usage bias were significantly  changed when the bacterial community underwent the treatment. </p> <p>Exercise 1: Reviewing metadata</p> <p>According to the results described for this CCB study:</p> <ol> <li> <p>What kind of sequencing method do you think they used, and why do you think so?    A) Metabarcoding    B) Shotgun metagenomics    C) Genomics of axenic cultures  </p> </li> <li> <p>In the table Samples treatment information,    what was the most critical piece of metadata that the authors took?</p> </li> </ol> Click to show the solution <p>1. Sequencing method</p> <p>A) Metabarcoding \u2014 \u274c False. With this technique, usually only one region of the genome is amplified. B) Shotgun Metagenomics \u2014 \u2705 True. Only shotgun metagenomics could have been used to investigate the total number of tRNA genes. C) Genomics of axenic cultures \u2014 \u274c False. Information on the microbial community cannot be fully obtained with axenic cultures.    </p> <p>2. Sequencing Metadata     The most crucial thing to know about our data is which community was and was not supplemented with fertilizers.     However, any differences in the technical parts of the study, such as the DNA extraction protocol,     could have affected the results, so tracking those is also essential.</p> <p>Exercise 3: Differentiate between IDs and sample names</p> <p>Depending on the database, several IDs can be used for the same sample. Please open the document where the metadata information is stored. Here, inspect the IDs and find out which of them correspond to sample JP4110514WATERRESIZE.</p> Click to show the solution <p>ERS1949771 is the SRA ID corresponding to JP4110514WATERRESIZE</p> <p>Exercise 4: Discuss the importance of metadata</p> <p>Which other information could you recommend to add in the metadata?</p> Click to show the solution <p>Metadata will depend on the type of the experiment, but some examples are: - Properties of the water before and after fertilization - Sampling and processing methodology - Date, time, and location (country, region, city, etc.)</p> <p>Throughout the lesson, we will use the first four  characters of the <code>File names (alias)</code> to identify the data files  corresponding to a sample. We are going to use the first two samples (JC1A and JP4D) for most of the lesson and the third one (JP41) for one exercise at the end.</p> SRA Accession File name (alias) Sample name in the lesson Treatment ERS1949784 JC1ASEDIMENT120627 JC1A Control mesocosm ERS1949801 JP4DASH2120627WATERAMPRESIZED JP4D Fertilized pond ERS1949771 JP4110514WATERRESIZE JP41 Unenriched pond <p>The results of this study, raw sequences, and metadata have  been submitted to the NCBI Sequence Read Archive (SRA) and stored in the BioProject PRJEB22811. </p> <p>SRA database is interconnected to the European Nucleotide Archive, which mean that you can access to the data on the ENA Webiste aswell:  Here is the link to JC1A: https://www.ebi.ac.uk/ena/browser/view/SAMEA104324806?show=reads .  Here is the link to JP4D: https://www.ebi.ac.uk/ena/browser/view/ERS1949801?dataType=SAMPLE&amp;show=reads . </p> <p>You can install the reads of JC1A and JP4D using these links:  https://zenodo.org/record/7871630/files/JC1A_R1.fastqsanger.gz https://zenodo.org/record/7871630/files/JC1A_R2.fastqsanger.gz https://zenodo.org/record/7871630/files/JP4D_R1.fastqsanger.gz https://zenodo.org/record/7871630/files/JP4D_R2.fastqsanger.gz </p>"},{"location":"pages/metagenomic/01-background-metadata/#other-metagenomic-databases","title":"Other metagenomic databases","text":"<p>The NCBI SRA is not the only repository for metagenomic information. There are other public metagenomic databases such as MG-RAST, MGnify, Marine Metagenomics Portal, Terrestrial Metagenome DB and the GM Repo. </p> <p>Each database requires certain metadata linked with the data. As an example, when <code>JP4D.fasta</code> is uploaded to  mg-RAST the associated metadata looks like this:</p> Column Description file_name JP4D.fasta investigation_type metagenome seq_meth Illumina project_description This project is a teaching project and uses data from Okie et al. Elife 2020 collection_date 2012-06-27 country Mexico feature pond water latitude 26.8717055555556 longitude -102.14 env_package water depth 0.165 <p>Key Points</p> <ul> <li>Shotgun metagenomics can be used for taxonomic and functional studies.</li> <li>Metabarcoding can be used for taxonomic studies.</li> <li>Collecting metadata beforehand is fundamental for downstream analysis.</li> <li>We will use data from a Cuatro Ci\u00e9negas project to learn about shotgun metagenomics.</li> </ul>"},{"location":"pages/metagenomic/02-fastq-desc/","title":"Assessing Read Quality","text":""},{"location":"pages/metagenomic/02-fastq-desc/#bioinformatic-workflows","title":"Bioinformatic workflows","text":"<p>When working with high-throughput sequencing data, the raw reads you get off the sequencer must pass through several different tools to generate your final desired output. The execution of this set of tools in a specified order is commonly referred to as a workflow or a pipeline. </p> <p>An example of the workflow we will be using for our analysis is provided below, with a brief description of each step. </p> <p> </p> <ol> <li>Quality control - Assessing quality using FastQC and Trimming and/or filtering reads (if necessary)</li> <li>Assembly of metagenome</li> <li>Binning</li> <li>Taxonomic assignation</li> </ol> <p>These workflows in bioinformatics adopt a plug-and-play approach in that the output of one tool can be easily used as input to another tool without any extensive configuration. Having standards for data formats is what  makes this feasible. Standards ensure that data is stored in a way that is generally accepted and agreed upon  within the community. Therefore, the tools used to analyze data at different workflow stages are built, assuming that the data will be provided in a specific format.  </p>"},{"location":"pages/metagenomic/02-fastq-desc/#quality-control","title":"Quality control","text":"<p>We will now assess the quality of the sequence reads contained in our FASTQ files. </p> <p> </p>"},{"location":"pages/metagenomic/02-fastq-desc/#details-on-the-fastq-format","title":"Details on the FASTQ format","text":"<p>Although it looks complicated (and it is), we can understand the FASTQ format with a little decoding. Some rules about the format include the following:  </p> Line Description 1 Always begins with '@' followed by the information about the read 2 The actual DNA sequence 3 Always begins with a '+' and sometimes contains the same info as in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 <p>We can view the first complete read in one of the files from our dataset using <code>head</code> to look at the first four lines. But we have to decompress one of the files first.</p> <p><pre><code>@MISEQ-LAB244-W7:156:000000000-A80CV:1:1101:12622:2006 1:N:0:CTCAGA\nCCCGTTCCTCGGGCGTGCAGTCGGGCTTGCGGTCTGCCATGTCGTGTTCGGCGTCGGTGGTGCCGATCAGGGTGAAATCCGTCTCGTAGGGGATCGCGAAGATGATCCGCCCGTCCGTGCCCTGAAAGAAATAGCACTTGTCAGATCGGAAGAGCACACGTCTGAACTCCAGTCACCTCAGAATCTCGTATGCCGTCTTCTGCTTGAAAAAAAAAAAAGCAAACCTCTCACTCCCTCTACTCTACTCCCTT                                        \n+                                                                                                \nA&gt;&gt;1AFC&gt;DD111A0E0001BGEC0AEGCCGEGGFHGHHGHGHHGGHHHGGGGGGGGGGGGGHHGEGGGHHHHGHHGHHHGGHHHHGGGGGGGGGGGGGGGGHHHHHHHGGGGGGGGHGGHHHHHHHHGFHHFFGHHHHHGGGGGGGGGGGGGGGGGGGGGGGGGGGGFFFFFFFFFFFFFFFFFFFFFBFFFF@F@FFFFFFFFFFBBFF?@;@#################################### \n</code></pre></p> <p>Line 4 shows the quality of each nucleotide in the read. Quality is interpreted as the  probability of an incorrect base call (e.g., 1 in 10) or, equivalently, the base call  accuracy (e.g., 90%). Each nucleotide's numerical score's value is converted into a character code where every single character  represents a quality score for an individual nucleotide. This conversion allows the alignment of each individual nucleotide with its quality score. For example, in the line above, the quality score line is: </p> <p><pre><code>A&gt;&gt;1AFC&gt;DD111A0E0001BGEC0AEGCCGEGGFHGHHGHGHHGGHHHGGGGGGGGGGGGGHHGEGGGHHHHGHHGHHHGGHHHHGGGGGGGGGGGGGGGGHHHHHHHGGGGGGGGHGGHHHHHHHHGFHHFFGHHHHHGGGGGGGGGGGGGGGGGGGGGGGGGGGGFFFFFFFFFFFFFFFFFFFFFBFFFF@F@FFFFFFFFFFBBFF?@;@#################################### \n</code></pre></p> <p>The numerical value assigned to each character depends on the  sequencing platform that generated the reads. The sequencing machine used to generate our data  uses the standard Sanger quality PHRED score encoding, using Illumina version 1.8 onwards. Each character is assigned a quality score between 0 and 41, as shown in  the chart below.</p> <p><pre><code>Quality encoding: !\"#$%&amp;'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJ\n                   |         |         |         |         |\nQuality score:    01........11........21........31........41                                \n</code></pre></p> <p>Each quality score represents the probability that the corresponding nucleotide call is incorrect. These probability values are the results of the base calling algorithm and depend on how  much signal was captured for the base incorporation. This quality score is logarithmically based, so a quality score of 10 reflects a base call accuracy of 90%, but a quality score of 20 reflects a base call accuracy of 99%.  In this  link you can find more information  about quality scores.</p> <p>Looking back at our read: </p> <p><pre><code>@MISEQ-LAB244-W7:156:000000000-A80CV:1:1101:12622:2006 1:N:0:CTCAGA\nCCCGTTCCTCGGGCGTGCAGTCGGGCTTGCGGTCTGCCATGTCGTGTTCGGCGTCGGTGGTGCCGATCAGGGTGAAATCCGTCTCGTAGGGGATCGCGAAGATGATCCGCCCGTCCGTGCCCTGAAAGAAATAGCACTTGTCAGATCGGAAGAGCACACGTCTGAACTCCAGTCACCTCAGAATCTCGTATGCCGTCTTCTGCTTGAAAAAAAAAAAAGCAAACCTCTCACTCCCTCTACTCTACTCCCTT                                        \n+                                                                                                \nA&gt;&gt;1AFC&gt;DD111A0E0001BGEC0AEGCCGEGGFHGHHGHGHHGGHHHGGGGGGGGGGGGGHHGEGGGHHHHGHHGHHHGGHHHHGGGGGGGGGGGGGGGGHHHHHHHGGGGGGGGHGGHHHHHHHHGFHHFFGHHHHHGGGGGGGGGGGGGGGGGGGGGGGGGGGGFFFFFFFFFFFFFFFFFFFFFBFFFF@F@FFFFFFFFFFBBFF?@;@#################################### \n</code></pre></p> <p>We can now see that there is a range of quality scores but that the end of the sequence is very poor (<code>#</code> = a quality score of 2). </p>"},{"location":"pages/metagenomic/02-fastq-desc/#assessing-read-quality-using-fastqe","title":"Assessing read quality using FASTqe","text":"<p>This symbol can be hard to interpet, that's why we are going to use our first tool to have a better understanding of the data: fastqE.  FastQE turns ASCII characters into emojis that are easy to interpret.</p> <p>We are going to use a Bioinformatic tool named Galaxy.</p>"},{"location":"pages/metagenomic/03-assessing-read-quality/","title":"Assessing Read Quality","text":""},{"location":"pages/metagenomic/03-assessing-read-quality/#assessing-quality-using-fastqc","title":"Assessing quality using FastQC","text":"<p>FastQC has several features that can give you a quick impression of any problems your data may have, so you can consider these issues before moving forward with your analyses. Rather than looking at quality scores for each read, FastQC looks at quality collectively across all reads within a sample. The image below shows one FastQC-generated plot that indicates a very high-quality sample:</p> <p> </p> <p>The x-axis displays the base position in the read, and the y-axis shows quality scores. In this  example, the sample contains reads that are 40 bp long. This length is much shorter than the reads we  are working on within our workflow. For each position, there is a box-and-whisker plot showing  the distribution of quality scores for all reads at that position. The horizontal red line  indicates the median quality score, and the yellow box shows the 1st to  3rd quartile range. This range means that 50% of reads have a quality score that falls within the  range of the yellow box at that position. The whiskers show the whole range covering  the lowest (0th quartile) to highest (4th quartile) values.</p> <p>The quality values for each position in this sample do not drop much lower than 32, which is a high-quality score. The plot background is also color-coded to identify good (green), acceptable (yellow) and bad (red) quality scores.</p> <p>Now let's look at a quality plot on the other end of the spectrum. </p> <p> </p> <p>The FastQC tool produces several other diagnostic plots to assess sample quality and the one plotted above. Here, we see positions within the read in which the boxes span a much more comprehensive range. Also, quality scores drop pretty low into the \"bad\" range, particularly on the tail end of the reads. </p>"},{"location":"pages/metagenomic/03-assessing-read-quality/#running-fastqc","title":"Running FastQC","text":"<p>We will now assess the quality of the reads that we downloaded</p>"},{"location":"pages/metagenomic/03-assessing-read-quality/#viewing-the-fastqc-results","title":"Viewing the FastQC results","text":"<p>Click on the galaxy file </p> <p>Now we can open the 4 HTML files. </p> <p>Depending on your system,  you should be able to select and open them all at once via a right-click menu in your file browser.</p> <p>## Exercise 4: Discuss the quality of sequencing files</p> <p>Discuss your results with a neighbor. Which sample(s) looks the best  per base sequence quality? Which sample(s) look the  worst?</p> <p>## Solution  All of the reads contain usable data, but the quality decreases toward  the end of the reads. File JC1A_R2_fastqc shows the lowest quality.  {: .solution}</p>"},{"location":"pages/metagenomic/03-assessing-read-quality/#decoding-the-other-fastqc-outputs","title":"Decoding the other FastQC outputs","text":"<p>We've now looked at quite a few \"Per base sequence quality\" FastQC graphs, but there are nine other graphs that we haven't talked about! Below we have provided a brief overview of interpretations for each plot. For more information, please see the FastQC documentation here </p> <ul> <li>Per tile sequence quality: the machines that perform sequencing are divided into tiles. This plot displays patterns in base quality along these tiles. Consistently low scores are often found around the edges, but hot spots could also occur in the middle if an air bubble was introduced during the run. </li> <li>Per sequence quality scores: a density plot of quality for all reads at all positions. This plot shows what quality scores are most common. </li> <li>Per base sequence content: plots the proportion of each base position over all of the reads. Typically, we expect to see each base roughly 25% of the time at each position, but this often fails at the beginning or end of the read due to quality or adapter content.</li> <li>Per sequence GC content: a density plot of average GC content in each of the reads.  </li> <li>Per base N content: the percent of times that 'N' occurs at a position in all reads. If there is an increase at a particular position, this might indicate that something went wrong during sequencing.  </li> <li>Sequence Length Distribution: the distribution of sequence lengths of all reads in the file. If the data is raw, there is often a sharp peak; however, if the reads have been trimmed, there may be a distribution of shorter lengths. </li> <li>Sequence Duplication Levels: a distribution of duplicated sequences. In sequencing, we expect most reads to only occur once. If some sequences are occurring more than once, it might indicate enrichment bias (e.g. from PCR). This might not be true if the samples are high coverage (or RNA-seq or amplicon).  </li> <li>Overrepresented sequences: a list of sequences that occur more frequently than would be expected by chance. </li> <li>Adapter Content: a graph indicating where adapter sequences occur in the reads.</li> <li>K-mer Content: a graph showing any sequences which may show a positional bias within the reads.</li> </ul>"},{"location":"pages/metagenomic/03-assessing-read-quality/#quality-of-large-datasets","title":"Quality of large datasets","text":"<p>Explore MultiQC if you want a tool that can show the quality of many samples at once.</p>"},{"location":"pages/metagenomic/03-assessing-read-quality/#quality-encodings-vary","title":"Quality Encodings Vary","text":"<p>Although we've used a particular quality encoding system to demonstrate the interpretation of  read quality, different sequencing machines use different encoding systems. This means that  depending on which sequencer you use to generate your data, a <code>#</code> may not indicate  a poor quality base call.</p> <p>This mainly relates to older Solexa/Illumina data. However, it's essential that you know which sequencing platform was used to generate your data to tell your quality control program which encoding to use. If you choose the wrong encoding, you run the risk of throwing away good reads or  (even worse) not throwing away bad reads!</p>"},{"location":"pages/metagenomic/04-trimming-filtering/","title":"Trimming and Filtering","text":""},{"location":"pages/metagenomic/04-trimming-filtering/#cleaning-reads","title":"Cleaning reads","text":"<p>In the last episode, we took a high-level look at the quality of each of our samples using <code>FastQC</code>. We visualized per-base quality graphs showing the distribution of the quality at each base across all the reads from our sample. This information helps us to determine  the quality threshold we will accept, and thus, we saw information about which samples fail which quality checks. Some of our samples failed  quite a few quality metrics used by FastQC. However, this does not mean that our samples should be thrown out! It is common to have some  quality metrics fail, which may or may not be a problem for your  downstream application. For our workflow, we will remove some low-quality sequences to reduce our false-positive rate due to  sequencing errors.</p> <p>To accomplish this, we will use a program called Trimmomatic. This  useful tool filters poor quality reads and trims poor quality bases  from the specified samples.</p>"},{"location":"pages/metagenomic/04-trimming-filtering/#trimmomatic-options","title":"Trimmomatic options","text":"<p>Trimmomatic has a variety of options to accomplish its task.  If we run the following command, we can see some of its options:</p> <p><pre><code>$ trimmomatic\n</code></pre></p> <p>Which will give you the following output: <pre><code>Usage: \n       PE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-summary &lt;statsSummaryFile&gt;] [-quiet] [-validatePairs] [-basein &lt;inputBase&gt; | &lt;inputFile1&gt; &lt;inputFile2&gt;] [-baseout &lt;outputBase&gt; | &lt;outputFile1P&gt; &lt;outputFile1U&gt; &lt;outputFile2P&gt; &lt;outputFile2U&gt;] &lt;trimmer1&gt;...\n   or: \n       SE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-summary &lt;statsSummaryFile&gt;] [-quiet] &lt;inputFile&gt; &lt;outputFile&gt; &lt;trimmer1&gt;...\n   or: \n       -version\n</code></pre></p> <p>This output shows that we must first specify whether we have paired-end (<code>PE</code>) or single-end (<code>SE</code>) reads. Next, we will specify with which flags we  want to run Trimmomatic. For example, you can specify <code>threads</code>  to indicate the number of processors on your computer that you want Trimmomatic  to use. In most cases, using multiple threads(processors) can help to run the  trimming faster. These flags are unnecessary, but they can give you more control over the command. The flags are followed by positional arguments, meaning  the order in which you specify them is essential. In paired-end mode, Trimmomatic  expects the two input files and then the names of the output files. These files are  described below. While in single-end mode, Trimmomatic will expect one file  as input, after which you can enter the optional settings and, lastly, the  name of the output file.</p> Option Meaning \\ input forward reads to be trimmed. Typically the file name will contain an <code>_1</code> or <code>_R1</code> in the name. \\ Input reverse reads to be trimmed. Typically the file name will contain an <code>_2</code> or <code>_R2</code> in the name. \\ Output file that contains surviving pairs from the <code>_1</code> file. \\ Output file that contains orphaned reads from the <code>_1</code> file. \\ Output file that contains surviving pairs from the <code>_2</code> file. \\ Output file that contains orphaned reads from the <code>_2</code> file. <p>The last thing Trimmomatic expects to see is the trimming parameters:</p> step meaning <code>ILLUMINACLIP</code> Perform adapter removal. <code>SLIDINGWINDOW</code> Perform sliding window trimming, cutting once the average quality within the window falls below a threshold. <code>LEADING</code> Cut bases off the start of a read if below a threshold quality. <code>TRAILING</code> Cut bases off the end of a read if below a threshold quality. <code>CROP</code> Cut the read to a specified length. <code>HEADCROP</code> Cut the specified number of bases from the start of the read. <code>MINLEN</code> Drop an entire read if it is below a specified length. <code>TOPHRED33</code> Convert quality scores to Phred-33. <code>TOPHRED64</code> Convert quality scores to Phred-64. <p>Understanding the steps you are using to clean your data is essential. We will use only a few options and trimming steps in our analysis. For more information about the Trimmomatic arguments and options, see the Trimmomatic manual.</p> <p> </p> <p>However, a complete command for Trimmomatic will look something like the command below. This command is an example and will not work, as we do not have the files it refers to:</p> <p><pre><code>$ trimmomatic PE -threads 4 SRR_1056_1.fastq SRR_1056_2.fastq \\\n              SRR_1056_1.trimmed.fastq SRR_1056_1un.trimmed.fastq \\\n              SRR_1056_2.trimmed.fastq SRR_1056_2un.trimmed.fastq \\\n              ILLUMINACLIP:SRR_adapters.fa SLIDINGWINDOW:4:20\n</code></pre></p> <p>In this example, we have told Trimmomatic:</p> code meaning <code>PE</code> that it will be taking a paired-end file as input <code>-threads 4</code> to use four computing threads to run (this will speed up our run) <code>SRR_1056_1.fastq</code> the first input file name. Forward <code>SRR_1056_2.fastq</code> the second input file name. Reverse <code>SRR_1056_1.trimmed.fastq</code> the output file for surviving pairs from the <code>_1</code> file <code>SRR_1056_1un.trimmed.fastq</code> the output file for orphaned reads from the <code>_1</code> file <code>SRR_1056_2.trimmed.fastq</code> the output file for surviving pairs from the <code>_2</code> file <code>SRR_1056_2un.trimmed.fastq</code> the output file for orphaned reads from the <code>_2</code> file <code>ILLUMINACLIP:SRR_adapters.fa</code> to clip the Illumina adapters from the input file using the adapter sequences listed in <code>SRR_adapters.fa</code> <code>SLIDINGWINDOW:4:20</code> to use a sliding window of size 4 that will remove bases if their Phred score is below 20"},{"location":"pages/metagenomic/04-trimming-filtering/#running-trimmomatic-on-galaxy","title":"Running Trimmomatic on Galaxy","text":"<p>Now, we will run Trimmomatic on our data. Instead of using a command, we are going to use Galaxy again !</p> <p>We are going to run Trimmomatic on one of our paired-end samples. First, we will do it with JP4D.</p>"},{"location":"pages/metagenomic/04-trimming-filtering/#create-a-new-history","title":"Create a new history","text":"<p>First, let's create a new history, and name it \"Trimming and Filtering\" (Ajouter une image de creation d'history). Then using the History Multiview, we can move the fastq from the previous analysis (FastQC). (JP4D collection) Then, we can type \"Trimmomatic\" inside the \"Tools\" .  </p> <p>While using FastQC, we saw that Universal adapters were present  in our samples. The adapter sequences came with the installation of  Trimmomatic and it is located in our current directory in the database  <code>TruSeq3</code>.</p> <p>We will also use a sliding window of size 4 that will remove bases if their Phred score is below 20 (like in our example above). We will also discard any reads that do not have at least 25 bases remaining after this trimming step. (Put a picture of the parameters to use inside Trimmomatic)</p> <p>Once you have selected all the good parameters, you can click on \"Run\". This command will take a few minutes to run. Open the output \" Trimmomatic on collection X (log file)</p> <p><pre><code>TrimmomaticPE: Started with arguments:\n JP4D_R1.fastq.gz JP4D_R2.fastq.gz JP4D_R1.trim.fastq.gz JP4D_R1un.trim.fastq.gz JP4D_R2.trim.fastq.gz JP4D_R2un.trim.fastq.gz SLIDINGWINDOW:4:20 MINLEN:35 ILLUMINACLIP:TruSeq3-PE.fa:2:40:15\nMultiple cores found: Using 2 threads\nUsing PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\nILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\nQuality encoding detected as phred33\nInput Read Pairs: 1123987 Both Surviving: 751427 (66.85%) Forward Only Surviving: 341434 (30.38%) Reverse Only Surviving: 11303 (1.01%) Dropped: 19823 (1.76%)\nTrimmomaticPE: Completed successfully\n</code></pre></p> <p>You may have noticed that Trimmomatic automatically detected the quality encoding of our sample (phred33). It is always a good idea to double-check this or manually enter the quality encoding.</p>"},{"location":"pages/metagenomic/04-trimming-filtering/#exercise-1-what-did-trimmomatic-do","title":"Exercise 1: What did Trimmomatic do?","text":"<p>Use the output from your Trimmomatic command to answer the following questions.</p> <p>1) What percent of reads did we discard from our sample? 2) What percent of reads did we keep both pairs?</p>"},{"location":"pages/metagenomic/04-trimming-filtering/#solution","title":"Solution","text":"<p>1) 1.76%   2) 66.85% {: .solution}</p>"},{"location":"pages/metagenomic/04-trimming-filtering/#exercise-2","title":"Exercise 2","text":"<p>The output files are also FASTQ files. </p> <p>Should the output fastqfile file be smaller or bigger than the input file</p> <pre><code>??? success \"It should be smaller than our input file because we have removed reads.\"\n</code></pre> <p>We have just successfully run Trimmomatic on one of our FASTQ files! Now we need to do it on our other sample : JC1A.</p> <p>We have completed the trimming and filtering steps of our quality control process! </p> <p>Rerun the FastQC Analysis to check if our sequences have been succesfully filtered and trimmed.</p> <p>After trimming and filtering, our overall quality is much higher,    we have a distribution of sequence lengths, and more samples pass    adapter content. However, quality trimming is not perfect, and some   programs are better at removing some sequences than others. Trimmomatic    did pretty well, though, and its performance is good enough for our workflow.</p> <p>Now that the Quality Control Process step is done, we can move on to the Taxoxomic assignement of our samples.   </p>"},{"location":"pages/metagenomic/04-trimming-filtering/#bonus-exercise-quality-test-after-trimming","title":"Bonus Exercise: Quality test after trimming","text":"<p>Now that our samples have gone through quality control, they should perform better on the quality tests run by FastQC. </p>"},{"location":"pages/metagenomic/05-taxonomic/","title":"Taxonomic Profiling","text":"<p>The investigation of microorganisms present at a specific site and their relative abundance is also called \u201cmicrobial community profiling\u201d. The main objective is to identify the microorganisms that are present within the given sample. This can be achieved for all known microbes, where the DNA sequence specific for a certain species is known.</p> <p>For that we try to identify the taxon to which each individual read belongs.</p>"},{"location":"pages/metagenomic/05-taxonomic/#what-is-taxonomy","title":"What is Taxonomy ?","text":"<p>Taxonomy is the method used to naming, defining (circumscribing) and classifying groups of biological organisms based on shared characteristics such as morphological characteristics, phylogenetic characteristics, DNA data, etc. It is founded on the concept that the similarities descend from a common evolutionary ancestor.</p> <p>Defined groups of organisms are known as taxa. Taxa are given a taxonomic rank and are aggregated into super groups of higher rank to create a taxonomic hierarchy. The taxonomic hierarchy includes eight levels: Domain, Kingdom, Phylum, Class, Order, Family, Genus and Species.</p>"},{"location":"pages/metagenomic/05-taxonomic/#what-is-a-taxonomic-assignment","title":"What is a taxonomic assignment?","text":"<p>A taxonomic assignment is a process of assigning an Operational Taxonomic Unit (OTU, that is, groups of related individuals) to sequences that can be  reads or contigs. Sequences are compared against a database constructed using complete genomes. When a sequence finds a good enough match in the database, it is assigned to the corresponding OTU. The comparison can be made in different ways.  </p>"},{"location":"pages/metagenomic/05-taxonomic/#strategies-for-taxonomic-assignment","title":"Strategies for taxonomic assignment","text":"<p>There are many programs for doing taxonomic mapping,  and almost all of them follow one of the following strategies:  </p> <ol> <li> <p>BLAST: Using BLAST or DIAMOND, these mappers search for the most likely hit  for each sequence within a database of genomes (i.e., mapping). This strategy is slow.    </p> </li> <li> <p>Markers: They look for markers of a database made a priori in the sequences  to be classified and assigned the taxonomy depending on the hits obtained.  </p> </li> <li> <p>K-mers: A genome database is broken into pieces of length k to be able to search for unique pieces by taxonomic group, from a lowest common ancestor (LCA),  passing through phylum to species. Then, the algorithm  breaks the query sequence (reads/contigs) into pieces of length k, looks for where these are placed within the tree and make the  classification with the most probable position.  </p> </li> </ol> <p>  Figure 1. Lowest common ancestor assignment example."},{"location":"pages/metagenomic/05-taxonomic/#abundance-bias","title":"Abundance bias","text":"<p>When you do the taxonomic assignment of metagenomes, a key result is the abundance of each taxon or OTU in your sample.   The absolute abundance of a taxon is the number of sequences (reads or contigs, depending on what you did) assigned to it.   Moreover, its relative abundance is the proportion of sequences assigned to it. It is essential to be aware of the many biases that can skew the  abundances along the metagenomics workflow, shown in the figure, and that because of them, we may not be obtaining the actual abundance of  the organisms in the sample.</p> <p> Figure 2. Abundance biases during a metagenomics protocol."},{"location":"pages/metagenomic/05-taxonomic/#discussion-taxonomic-level-of-assignment","title":"Discussion: Taxonomic level of assignment","text":"<p>What do you think is harder to assign, a species (like E. coli) or a phylum (like Proteobacteria)?</p>"},{"location":"pages/metagenomic/05-taxonomic/#data-used","title":"Data used","text":"<p>we will use the 2 datasets that we previously filtered and trimmed:</p> <p>JP4D: a microbiome sample collected from the Lagunita Fertilized Pond JC1A: a control samples from a control mesocosm.</p>"},{"location":"pages/metagenomic/05-taxonomic/#using-k-mer-based-approach-with-the-tool-kraken-2","title":"Using k-mer based approach, with the tool Kraken 2","text":"<p>Kraken 2 is the newest version of Kraken,  a taxonomic classification system using exact k-mer matches to achieve  high accuracy and fast classification speeds. </p> <p>Kraken 2 is available on Galaxy. We will follow this tutorial :  (https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/taxonomic-profiling/tutorial.html)</p>"},{"location":"pages/metagenomic/05-taxonomic/#compute-the-species-abundance-using-bracken","title":"Compute the species abundance, using Bracken","text":"<p>Several accurate methods have appeared that can align a sequence \u201cread\u201d to a database of microbial genomes rapidly and accurately (like Kraken2), but this step alone is not sufficient to estimate how much of a species is present. Complications arise when closely related species are present in the same sample\u2013a situation that arises quite frequently\u2013because many reads align equally well to more than one species. This requires a separate abundance estimation algorithm to resolve. </p> <p>How it works : - 1)You first run Kraken, which assigns each DNA read to a species, genus, or higher-level taxon. - 2) Bracken takes the initial classification from Kraken and adjusts the counts to give a more accurate estimate of the number of reads from each species. Bracken looks at the assignments from Kraken and uses a statistical model to estimate the true abundance of species. 3) It outputs a table of species names and estimated counts or proportions.</p> <p> Figure 2. Uses of Bracken on true abundace reestimation"},{"location":"pages/metagenomic/06-diversity/","title":"Diversity Tackled With R","text":""},{"location":"pages/metagenomic/06-diversity/#first-plunge-into-diversity","title":"First plunge into diversity","text":"<p>Species diversity, in its simplest definition, is the number of species in a particular area and their relative abundance (evenness). Once we know the taxonomic composition of our metagenomes, we can do diversity analyses.  Here we will discuss the two most used diversity metrics, \u03b1 diversity (within one metagenome) and \u03b2 (across metagenomes).   </p> <ul> <li>\u03b1 Diversity: \u03b1 Diversity describes the diversity within a community.  It considers the number of different species in an environment (also referred to as species richness).Additionally, it can take the abundance of each species into account to measure how evenly individuals are distributed across the sample (also referred to as species evenness). To measure \u03b1-diversity, we use indexes such as Shannon's, Simpson's, Chao1, etc.</li> </ul> <p>  Figure 1. Alpha diversity is calculated according to fish diversity in a pond. Here, alpha diversity is represented in its simplest way: Richness.  <ul> <li>\u03b2 compare the diversity between different communities by measuring their distance.  It can be measured with metrics like Bray-Curtis dissimilarity, Jaccard distance, or UniFrac distance, to name a few. Each one  of this measures are focused on a characteristic of the community (e.g., Unifrac distance measures the phylogenetic relationship between the species of the community).</li> </ul> <p>In the next example, we will look at the \u03b1 and the \u03b2 components of the diversity of a  dataset of fishes in three lakes. The most simple way to calculate the \u03b2-diversity  is to calculate the distinct species between two lakes (sites). Let us take as an example the diversity between Lake A and Lake B. The number of species in Lake A is 3. To this quantity, we will subtract the number of these species that are shared with the Lake  B: 2. So the number of unique species in Lake A compared to Lake B is (3-2) = 1. To  this number, we will sum the result of the same operations but now take Lake B as  our reference site. In the end, the \u03b2 diversity between Lake A and Lake B is  (3-2) + (3-2) = 2. This process can be repeated, taking each pair of lakes as the  focused sites.</p> <p>  Figure 2. Alpha and beta diversity indexes of fishes in a pond. <p>If you want to read more about diversity, we recommend to you this paper on  the concept of diversity. </p> <p>Exercise 1: Simple measure of alpha and beta diversities.</p> <p>In the next picture, there are two lakes with different fish species:</p> <p> </p> <p>Figure 3.</p> <p>Which of the options below is true for the alpha diversity in lakes A, B, and beta diversity between lakes A and B, respectively?</p> <ol> <li><code>4, 3, 1</code> </li> <li><code>4, 3, 5</code> </li> <li><code>9, 7, 16</code></li> </ol> Solution <p>Answer: 4, 3, 5</p> <p>Alpha diversity here is the number of different species in each lake.  </p> <ul> <li>Lake A has 4 different species.  </li> <li>Lake B has 3 different species.</li> </ul> <p>Beta diversity (using the formula from Figure 2) is the count of species unique to each lake summed together:</p> <ul> <li>Shared species between A and B = 1.</li> <li>Species unique to A = <code>4 - 1 = 3</code>.</li> <li>Species unique to B = <code>3 - 1 = 2</code>.</li> <li>Beta diversity = <code>3 + 2 = 5</code>. </li> </ul>"},{"location":"pages/metagenomic/06-diversity/#diversity","title":"\u03b1 diversity","text":"<p>There are several different indexes used to calculate \u03b1 diversity because different indexes capture different aspects of diversity and have varying sensitivities to different factors. These indexes have been developed to address specific research questions, account for different ecological or population characteristics, or highlight certain aspects of diversity.</p> Diversity Indices Description Shannon (H) Estimation of species richness and species evenness. More weight on richness. Simpson\u2019s (D) Estimation of species richness and species evenness. More weight on evenness. Chao1 Abundance based on species represented by a single individual (singletons) and two individuals (doubletons). <ul> <li>Shannon (H): </li> </ul> Variable Definition Definition Number of OTUs The proportion of the community represented by OTU i <ul> <li>Simpson's (D) </li> </ul> Variable Definition Definition Total number of the species in the community Proportion of community represented by OTU i <p>Simpsons index calculates the probability that two individuals selected from a community will be of the same species. Small values are observed in datasets of high diversity and large values in datasets of low diversity. The index ranges from 0 to 1, with 1 representing minimum diversity (1 specie). A value of 0.6 indicates that two randomly selected individuals from the community have 60% probability to belong to different species.  </p> <ul> <li>Chao1  </li> </ul> Variable Definition Definition the number of species represented by a single individual (singletons) the number of species represented by two individuals (doubletons) The number of observed species <p>Chao1 estimates the true species richness or diversity of a community, particularly when there might be rare or unobserved species. Chao1 estimates the number of unobserved species based on the number of singletons and doubletons. It assumes that there are additional rare species that are likely to exist but have not been observed. The estimation considers the number of unobserved singletons and doubletons and incorporates them into the observed species richness to provide an estimate of the true species richness.  </p>"},{"location":"pages/metagenomic/06-diversity/#diversity_1","title":"\u03b2 diversity","text":"<p>Diversity \u03b2 measures how different two or more communities are, either in their composition (richness) or in the abundance of the organisms that compose it (abundance). </p> <ul> <li>Bray-Curtis dissimilarity</li> </ul> Variable Definition $ BC_{ij} = 1 - \\frac{2C_{ij}}{S_{i} + S_{j}} Definition the sum of the absolute differences in abundances between corresponding species in samples i and j the total abundance or sum of species abundances in sample i the total abundance or sum of species abundances in sample j <p>Bray-Curtis dissimilarity measures The difference in richness and abundance across environments (samples). Weight on abundance. Measures the differences  The higher the dissimilarity value, the greater the difference in species composition or abundances. Goes from 0 (equal communities) to 1 (different communities)</p> <ul> <li>Jaccard distance</li> </ul> Variable Definition Definition the intersection of sets X and Y, i.e. elements common to both sets the union of sets X and Y, i.e. all unique elements from both sets combined <p>Jaccard distance is based on the presence/absence of species (diversity).  It goes from 0 (same species in the community) to 1 (no species in common)</p> <ul> <li>UniFrac</li> </ul> <p>UniFrac Measures the phylogenetic distance; how alike the trees in each community are.  There are two types, without weights (diversity) and with weights (diversity and abundance)    Figure 3. UniFrac Score explained. <p>There are different ways to plot and show the results of such analysis. Among others, PCA, PCoA, or NMDS analysis are widely used.</p>"},{"location":"pages/metagenomic/06-diversity/#compute-the-alpha-diversity","title":"Compute the Alpha diversity","text":"<p>Now that we have seen the definition of Alpha and Beta diversity. Let's try to compute some of this indices. We can do it using KrakenTools in Galaxy. Let's follow this tutorial: https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/diversity/tutorial.html#evaluation-of-different-diversity-metrics</p>"},{"location":"pages/metagenomic/06-diversity/#create-a-new-history","title":"Create a new history","text":""},{"location":"pages/metagenomic/06-diversity/#discussion-indexes-of-diversity","title":"Discussion: Indexes of diversity","text":"<p>Why do you think we need different indexes to asses diversity? What index will you use to assess the impact of rare, low-abundance taxa?</p>"},{"location":"pages/metagenomic/06-diversity/#solution","title":"Solution","text":"<p>It will be difficult (if not impossible) to take two communities and observe the same distribution of all members. This outcome is because  there are a lot of factors affecting these lineages. Some of the environmental factors are temperature, pH, and nutrient concentration. Also, the interactions of these populations, such as competence, inhibition of other populations, and growth speed,  are an important driver of variation (biotic factor). A combination of the factors mentioned above, can interact to maintain some populations with low abundance (rare taxa In order to have  ways to assess hypotheses regarding which of these processes can be affecting the community, we use all these different indexes. Some emphasize the number of  species and other the evenness of the OTUs.  To assess the impact of low abundance lineages, one alpha diversity index widely used is the Chao1 index.</p>"},{"location":"pages/metagenomic/07-assembly/","title":"Metagenome Assembly","text":""},{"location":"pages/metagenomic/07-assembly/#assembling-reads","title":"Assembling reads","text":"<p>The assembly process groups reads into contigs and contigs into  scaffolds to obtain (ideally) the sequence of a whole  chromosome. There are many programs devoted to genome and  metagenome assembly, some of the main strategies they use are Greedy extension, OLC, and De Bruijn charts. Contrary to metabarcoding, shotgun metagenomics needs an assembly step, which does not mean that metabarcoding never uses an assembly step but sometimes is unnecessary.</p> <p> </p> <p>MetaSPAdes is an NGS de novo assembler  for assembling large and complex metagenomics data, and it is one of the  most used and recommended. It is part of the SPAdes toolkit, which  contains several assembly pipelines.</p> <p>Some of the problems faced by metagenomics assembly are:  Differences in coverage between the genomes due to the differences in abundance in the sample.  The fact that different species often share conserved regions. * The presence of several strains of a single species in the community.   </p> <p>SPAdes already deals with the non-uniform coverage problem in its algorithm, so it is helpful for the assembly of simple communities, but the metaSPAdes algorithm deals with the other problems as well, allowing it to assemble metagenomes from complex communities. </p> <p>The process of (metagenomics) assembly can take a long time, and if the connection to the server drops, the process is killed, and the process needs to restart. To avoid this, we can create a screen session.</p>"},{"location":"pages/metagenomic/07-assembly/#screen-sessions","title":"Screen sessions","text":""},{"location":"pages/metagenomic/07-assembly/#starting-a-new-session","title":"Starting a new session","text":"<p>A \u2018session\u2019 can be considered a new window or screen: you might open a terminal to do one thing on the computer and then open a new terminal to work on another task at the command line. You can start a session and give it a descriptive name:</p> <p><pre><code>$ screen -S assembly\n</code></pre></p> <p>This steps creates a session with the name \u2018assembly\u2019.</p> <p>As you work, this session will stay active until you close it. Even if you log out or work on something else, the jobs you start in this session will run until completion.</p>"},{"location":"pages/metagenomic/07-assembly/#detach-session-process-keeps-running-in-the-background","title":"Detach session (process keeps running in the background)","text":"<p>You can detach from a session by pressing <code>control + a</code> followed by <code>d</code> (for detach) on your keyboard. If you reconnect to your machine, you will have to reconnect to your session to see how it went.</p> <p>Let's see if our program is installed correctly: <pre><code>$ metaspades.py\n</code></pre></p> <p><pre><code>SPAdes genome assembler v3.15.0 [metaSPAdes mode]\n\nUsage: spades.py [options] -o &lt;output_dir&gt;\n\nBasic options:\n  -o &lt;output_dir&gt;             directory to store all the resulting files (required)\n  --iontorrent                this flag is required for IonTorrent data\n  --test                      runs SPAdes on a toy dataset\n  -h, --help                  prints this usage message\n  -v, --version               prints version\n\nInput data:\n  --12 &lt;filename&gt;             file with interlaced forward and reverse paired-end reads\n  -1 &lt;filename&gt;               file with forward paired-end reads\n  -2 &lt;filename&gt;               file with reverse paired-end reads    \n</code></pre></p>"},{"location":"pages/metagenomic/07-assembly/#additional-session-commands","title":"Additional session commands","text":"<p>Seeing active sessions If you disconnect from your session or from your ssh, you will need to reconnect to an existing  <code>screen</code> session. You can see a list of existing sessions: <pre><code>$ screen -ls\n</code></pre> {: .bash} Reconnecting to a session To reconnect to an existing session:</p> <p><pre><code>$ screen -r session_name\n</code></pre></p> <p>The <code>-r</code> option = 'resume  a detached screen session'</p> <p>Kill a session To end a session, type <code>exit</code> after reconnecting to the session:</p> <p><pre><code>$ screen -r session_name\n$ exit\n</code></pre> {: .bash}</p>"},{"location":"pages/metagenomic/07-assembly/#activate-your-environment","title":"Activate your environment","text":"<p>If you do not have the metagenomics environment activated, the previous command should have given you an error.  Before you proceed, activate the environment: <pre><code>conda activate metagenomics\n</code></pre> {: .language-bash}</p>"},{"location":"pages/metagenomic/07-assembly/#metaspades-is-a-metagenomics-assembler","title":"MetaSPAdes is a metagenomics assembler","text":"<p>The help we just saw tells us how to run <code>metaspades.py</code>. We are going  to use the most straightforward options, just specifying our forward paired-end  reads with <code>-1</code> and reverse paired-end reads with <code>-2</code>, and the output  directory where we want our results to be stored.   ~~~  metaspades.py -1 JC1A_R1.trim.fastq.gz -2 JC1A_R2.trim.fastq.gz -o ../../results/assembly_JC1A <pre><code>{: .bash}\n\nNow that it is running we should detach our screen with `control + a` `d` and wait for a few minutes while it running. And then attach the screen with \n`screen -r assembly` to see how it went.  \n\nWhen the run is finished, it shows this message:\n</code></pre> ======= SPAdes pipeline finished.</p> <p>SPAdes log can be found here: /home/dcuser/dc_workshop/results/assembly_JC1A/spades.log</p> <p>Thank you for using SPAdes!</p> <p><pre><code>{: .bash}\nNow we can kill the screen with `exit` and look at our results in the main screen.\n\nNow, let's go to the output files: \n</code></pre>  ls -F <pre><code>{: .bash}\n</code></pre> assembly_graph_after_simplification.gfa  corrected/              K55/             scaffolds.fasta assembly_graph.fastg                     dataset.info            misc/            scaffolds.paths assembly_graph_with_scaffolds.gfa        first_pe_contigs.fasta  params.txt       spades.log before_rr.fasta                          input_dataset.yaml      pipeline_state/  strain_graph.gfa contigs.fasta                            K21/                    run_spades.sh    tmp/ contigs.paths                            K33/                    run_spades.yaml  ~~~</p> <p>As we can see, MetaSPAdes gave us a lot of files. The ones with the assembly are the <code>contigs.fasta</code> and the <code>scaffolds.fasta</code>.  Also, we found three <code>K</code> folders: K21, K33, and K55; this contains the individual result files for an assembly  with k-mers equal to those numbers: 21, 33, and 55. The best-assembled results are  the ones that are displayed outside these k-folders. The folder <code>corrected</code> hold the corrected reads  with the SPAdes algorithm. Moreover, the file  <code>assembly_graph_with_scaffolds.gfa</code> have the information needed to visualize  our assembly by different means, like programs such as Bandage.</p> <p>The contigs are just made from assembled reads, but the scaffolds are the result  from a subsequent process in which the contigs are ordered, oriented, and connected with Ns.</p> <p>We can recognize which sample our assembly outputs corresponds to because they are inside  the assembly results folder: <code>assembly_JC1A/</code>. However, the files within it do not have the  sample ID. If we need the files out of their folder, it is beneficial to rename them.</p>"},{"location":"pages/metagenomic/07-assembly/#exercise-1-rename-all-files-in-a-folder-needed-in-the-next-episode","title":"Exercise 1: Rename all files in a folder (needed in the next episode)","text":"<p>Add the prefix <code>JC1A</code> (the sample ID) separated by a <code>_</code> to the beginning of the names of all the contents in the <code>assembly_JC1A/</code> directory. Remember that many solutions are possible.</p> <p>A) <code>$ mv * JC1A_</code>  B) <code>$ mv * JC1A_*</code>  C) <code>$ for name in *; do mv $name JC1A_; done</code>  D) <code>$ for name in *; do mv $name JC1A_$name; done</code> </p>"},{"location":"pages/metagenomic/07-assembly/#solution","title":"Solution","text":"<p>A)  No, this option is going to give you as error <code>mv: target 'JC1A_' is not a directory</code>   This is because <code>mv</code> has two options: <code>mv file_1 file_2</code> <code>mv file_1, file_2, ..... file_n directory</code>   When a list of files is passed to <code>mv</code>, the <code>mv</code> expects the last parameters to be a directory.  Here, <code>*</code> gives you a list of all the files in the directory. The last parameter is <code>JC1A_</code> (which <code>mv</code> expects to be a directory).   B)  No. Again, every file is sent to the same file.  C)  No, every file is sent to the same file JC1A_  D)  Yes, this is one of the possible solutions.  </p> <p>\u00bfDo you have another solution? {: .bash} {: .solution}</p>"},{"location":"pages/metagenomic/07-assembly/#exercise-2-compare-two-fasta-files-from-the-assembly-output","title":"Exercise 2: Compare two fasta files from the assembly output","text":"<p>You want to know how many contigs and scaffolds result from the assembly.  Use <code>contigs.fasta</code>  and <code>scaffolds.fasta</code> files and sort the commands to create correct code lines.  Do they have the same number of lines? Why? Hint: You can use the following commands: <code>grep</code>, <code>|</code> (pipe), <code>-l</code>, <code>\"&gt;\"</code>, <code>wc</code>, <code>filename.fasta</code></p> <p>{: .challenge}</p>"},{"location":"pages/metagenomic/07-assembly/#solution_1","title":"Solution","text":"<p>~~~  grep '&gt;' scaffolds.fasta | wc -l ~~~</p> <p>A contig is created from reads and then a scaffold from a group of contigs, so we expect fewer lines in the <code>scaffolds.fasta</code> .</p> <p>{: .solution}</p>"},{"location":"pages/metagenomic/07-assembly/#quality-of-assemblies","title":"Quality of assemblies","text":"<p>You can use several metrics to know the quality of your assemblies. MetaQuast is a program  that gives you these metrics for metagenome assemblies in an interactive report and text files and plots.</p>"},{"location":"pages/metagenomic/07-phyloseq/","title":"Macro Rendering Error","text":"<p>File: <code>pages/metagenomic/07-phyloseq.md</code></p> <p>UndefinedError: 'links' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/home/lucent/.local/lib/python3.10/site-packages/mkdocs_macros/plugin.py\", line 699, in render\n    return md_template.render(**page_variables)\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 298, in top-level template code\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 490, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'links' is undefined\n</code></pre>"},{"location":"pages/metagenomic/08-binning/","title":"Macro Rendering Error","text":"<p>File: <code>pages/metagenomic/08-binning.md</code></p> <p>UndefinedError: 'links' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/home/lucent/.local/lib/python3.10/site-packages/mkdocs_macros/plugin.py\", line 699, in render\n    return md_template.render(**page_variables)\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 194, in top-level template code\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 490, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'links' is undefined\n</code></pre>"},{"location":"pages/metagenomic/09-abundance-analyses/","title":"Macro Rendering Error","text":"<p>File: <code>pages/metagenomic/09-abundance-analyses.md</code></p> <p>UndefinedError: 'links' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/home/lucent/.local/lib/python3.10/site-packages/mkdocs_macros/plugin.py\", line 699, in render\n    return md_template.render(**page_variables)\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 286, in top-level template code\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 490, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'links' is undefined\n</code></pre>"},{"location":"pages/metagenomic/10-OtherResources/","title":"Macro Rendering Error","text":"<p>File: <code>pages/metagenomic/10-OtherResources.md</code></p> <p>UndefinedError: 'links' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/home/lucent/.local/lib/python3.10/site-packages/mkdocs_macros/plugin.py\", line 699, in render\n    return md_template.render(**page_variables)\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 89, in top-level template code\n  File \"/home/lucent/.local/lib/python3.10/site-packages/jinja2/environment.py\", line 490, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'links' is undefined\n</code></pre>"},{"location":"pages/metagenomic/fig/extrasMAGs/","title":"Index","text":"<p>Figuras para lecci\u00f3n extra de reconstrucci\u00f3n de genomas</p>"}]}